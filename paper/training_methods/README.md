# Training Methods - í•™ìŠµ ê¸°ë²•

> ğŸ’¡ **í•µì‹¬ ì§ˆë¬¸**: ì œí•œëœ GPU ë©”ëª¨ë¦¬ì—ì„œ ì–´ë–»ê²Œ ëŒ€í˜• ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµí•  ê²ƒì¸ê°€?

íš¨ìœ¨ì ì¸ íŒŒì¸íŠœë‹ê³¼ í•™ìŠµ ì „ëµì˜ ë°œì „ íë¦„ì„ ì •ë¦¬í•©ë‹ˆë‹¤.

---

## ğŸ¯ ì´ ì¹´í…Œê³ ë¦¬ì˜ ëª©í‘œ

7B íŒŒë¼ë¯¸í„° ëª¨ë¸ì„ Full Fine-tuning í•˜ë ¤ë©´ **112GB GPU ë©”ëª¨ë¦¬**ê°€ í•„ìš”í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ìš°ë¦¬ëŠ” **T4 (16GB)**ì—ì„œë„ í•™ìŠµí•´ì•¼ í•©ë‹ˆë‹¤!

```mermaid
flowchart LR
    subgraph Problem["âŒ ë¬¸ì œ"]
        Full["Full Fine-tuning<br/>7B ëª¨ë¸<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>í•„ìš”: 112GB<br/>ë³´ìœ : 16GB (T4)"]
    end

    subgraph Solution["âœ… í•´ê²°ì±…"]
        QLoRA["QLoRA<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>í•„ìš”: 6GB<br/>T4ì—ì„œ ê°€ëŠ¥!"]
    end

    Problem -->|"PEFT + ì–‘ìí™”"| Solution

    style Problem fill:#ffe3e3
    style Solution fill:#d3f9d8
```

---

## ğŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ

### 7B ëª¨ë¸ ê¸°ì¤€

```mermaid
xychart-beta
    title "í•™ìŠµ ë°©ë²•ë³„ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (GB)"
    x-axis ["Full FT", "LoRA", "QLoRA"]
    y-axis "ë©”ëª¨ë¦¬ (GB)" 0 --> 120
    bar [112, 56, 6]
```

### ì™œ ì´ë ‡ê²Œ ì°¨ì´ê°€ ë‚˜ëŠ”ê°€?

```mermaid
flowchart TB
    subgraph FullFT["Full Fine-tuning: 112GB"]
        F1["ëª¨ë¸ ê°€ì¤‘ì¹˜ (FP16)<br/>7B Ã— 2 bytes = 14GB"]
        F2["Gradients<br/>7B Ã— 2 bytes = 14GB"]
        F3["Optimizer States (Adam)<br/>7B Ã— 8 bytes = 56GB"]
        F4["Activations<br/>~28GB"]
        
        F1 --> Total1["ì´: ~112GB"]
        F2 --> Total1
        F3 --> Total1
        F4 --> Total1
    end

    subgraph QLoRA_Mem["QLoRA: 6GB"]
        Q1["ëª¨ë¸ ê°€ì¤‘ì¹˜ (4-bit)<br/>7B Ã— 0.5 bytes = 3.5GB"]
        Q2["LoRA ê°€ì¤‘ì¹˜<br/>~16M Ã— 2 bytes = 32MB"]
        Q3["Optimizer (LoRAë§Œ)<br/>~16M Ã— 8 bytes = 128MB"]
        Q4["Activations<br/>~2GB"]
        
        Q1 --> Total2["ì´: ~6GB"]
        Q2 --> Total2
        Q3 --> Total2
        Q4 --> Total2
    end

    style Total1 fill:#ffe3e3
    style Total2 fill:#d3f9d8
```

---

## ğŸ“ˆ PEFT ë°œì „ íë¦„

```mermaid
flowchart TB
    subgraph Era2021["2021ë…„: PEFTì˜ ì‹œì‘"]
        LoRA["ğŸ”§ LoRA<br/>Low-Rank Adaptation<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>â€¢ ê°€ì¤‘ì¹˜ë¥¼ ì €ì°¨ì›ìœ¼ë¡œ ë¶„í•´<br/>â€¢ 0.1~1% íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ<br/>â€¢ ë©”ëª¨ë¦¬ ~50% ì ˆì•½"]
    end

    subgraph Era2023["2023ë…„: ê·¹í•œì˜ íš¨ìœ¨"]
        QLoRA["âš¡ QLoRA<br/>4-bit + LoRA<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>â€¢ NF4 ì–‘ìí™”<br/>â€¢ Double Quantization<br/>â€¢ ë©”ëª¨ë¦¬ ~90% ì ˆì•½"]
    end

    subgraph Era2024["2024ë…„: í’ˆì§ˆ í–¥ìƒ"]
        DoRA["ğŸ“ˆ DoRA<br/>Weight-Decomposed<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>â€¢ Magnitude + Direction ë¶„ë¦¬<br/>â€¢ ê°™ì€ rë¡œ ë” ë‚˜ì€ ì„±ëŠ¥<br/>â€¢ ì•ˆì •ì  í•™ìŠµ"]
    end

    LoRA --> QLoRA
    LoRA --> DoRA

    subgraph Project["ğŸ¯ ìš°ë¦¬ ì„ íƒ"]
        Choice["T4/L4: QLoRA (í•„ìˆ˜)<br/>A100+: LoRA/DoRA (ì„ íƒ)"]
    end

    QLoRA ==> Choice
    DoRA -.-> Choice

    style QLoRA fill:#d3f9d8,stroke:#2f9e44,stroke-width:3px
    style Choice fill:#ff6b6b,stroke:#c92a2a,color:#fff
```

---

## ğŸ”¬ LoRA ìƒì„¸ ì„¤ëª…

### í•µì‹¬ ì•„ì´ë””ì–´

ì¼ë°˜ì ì¸ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ëŠ” **ì „ì²´ í–‰ë ¬**ì„ ìˆ˜ì •í•©ë‹ˆë‹¤. LoRAëŠ” ì´ë¥¼ **ì €ì°¨ì› í–‰ë ¬ì˜ ê³±**ìœ¼ë¡œ ê·¼ì‚¬í•©ë‹ˆë‹¤.

```mermaid
flowchart TB
    subgraph Original["ê¸°ì¡´ ë°©ì‹"]
        W["W (4096Ã—4096)<br/>= 16.7M íŒŒë¼ë¯¸í„°"]
        DW["Î”W (4096Ã—4096)<br/>= 16.7M í•™ìŠµ"]
        W --> |"W + Î”W"| WNew["W' (ì—…ë°ì´íŠ¸)"]
        DW --> WNew
    end

    subgraph LoRA_Way["LoRA ë°©ì‹"]
        W2["W (4096Ã—4096)<br/>Frozen â„ï¸"]
        A["A (4096Ã—16)<br/>= 65K í•™ìŠµ"]
        B["B (16Ã—4096)<br/>= 65K í•™ìŠµ"]
        W2 --> |"W + BÃ—A"| WNew2["W' (ì—…ë°ì´íŠ¸)"]
        A --> |"ì €ì°¨ì› ê³±"| BA["BÃ—A"]
        B --> BA
        BA --> WNew2
    end

    subgraph Compare["ë¹„êµ"]
        C["16.7M â†’ 130K<br/>íŒŒë¼ë¯¸í„° 99% ê°ì†Œ!"]
    end

    style W fill:#ffe3e3
    style DW fill:#ffe3e3
    style A fill:#d3f9d8
    style B fill:#d3f9d8
    style C fill:#fff3bf
```

### ìˆ˜í•™ì  í‘œí˜„

```
ì›ë³¸:      h = W Ã— x
LoRA:     h = W Ã— x + (B Ã— A) Ã— x Ã— (Î±/r)

ì—¬ê¸°ì„œ:
â€¢ W: ì›ë³¸ ê°€ì¤‘ì¹˜ (frozen)
â€¢ A: 4096 Ã— r í–‰ë ¬ (í•™ìŠµ) - Down-projection
â€¢ B: r Ã— 4096 í–‰ë ¬ (í•™ìŠµ) - Up-projection
â€¢ r: rank (ë³´í†µ 8~64)
â€¢ Î±: scaling factor (ë³´í†µ 2Ã—r)
```

### Rank ì„ íƒ ê°€ì´ë“œ

```mermaid
flowchart TB
    subgraph Ranks["Rankë³„ íŠ¹ì„±"]
        R8["r=8<br/>â”€â”€â”€â”€â”€â”€â”€â”€<br/>íŒŒë¼ë¯¸í„°: ìµœì†Œ<br/>ë©”ëª¨ë¦¬: ìµœì†Œ<br/>í’ˆì§ˆ: ê¸°ë³¸"]
        R16["r=16<br/>â”€â”€â”€â”€â”€â”€â”€â”€<br/>íŒŒë¼ë¯¸í„°: ì ìŒ<br/>ë©”ëª¨ë¦¬: ì ìŒ<br/>í’ˆì§ˆ: ì¢‹ìŒ"]
        R32["r=32<br/>â”€â”€â”€â”€â”€â”€â”€â”€<br/>íŒŒë¼ë¯¸í„°: ë³´í†µ<br/>ë©”ëª¨ë¦¬: ë³´í†µ<br/>í’ˆì§ˆ: ë§¤ìš° ì¢‹ìŒ"]
        R64["r=64<br/>â”€â”€â”€â”€â”€â”€â”€â”€<br/>íŒŒë¼ë¯¸í„°: ë§ìŒ<br/>ë©”ëª¨ë¦¬: ë§ìŒ<br/>í’ˆì§ˆ: ìµœê³ "]
    end

    subgraph GPU_Rec["GPUë³„ ê¶Œì¥"]
        T4["T4 â†’ r=8"]
        L4["L4 â†’ r=16"]
        A100["A100 â†’ r=32"]
        H100["H100 â†’ r=64"]
    end

    R8 --> T4
    R16 --> L4
    R32 --> A100
    R64 --> H100

    style R8 fill:#fff3bf
    style R16 fill:#d3f9d8
    style R32 fill:#4dabf7
    style R64 fill:#e5dbff
```

---

## âš¡ QLoRA ìƒì„¸ ì„¤ëª…

### NF4 (NormalFloat 4-bit)

ì¼ë°˜ì ì¸ INT4ëŠ” **ê· ì¼í•œ ê°„ê²©**ìœ¼ë¡œ ì–‘ìí™”í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì‹¤ì œ ê°€ì¤‘ì¹˜ëŠ” **ì •ê·œë¶„í¬**ë¥¼ ë”°ë¦…ë‹ˆë‹¤!

```mermaid
flowchart TB
    subgraph INT4["INT4 ì–‘ìí™”"]
        I_Dist["ê· ì¼ ë¶„í¬ ê°€ì •<br/>[-8, -7, ..., 6, 7]<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>ì‹¤ì œ ë¶„í¬ì™€ ë¶ˆì¼ì¹˜<br/>ì–‘ìí™” ì˜¤ë¥˜ í¼"]
    end

    subgraph NF4_Q["NF4 ì–‘ìí™”"]
        N_Dist["ì •ê·œ ë¶„í¬ ê°€ì •<br/>[-1.0, -0.69, -0.52, ..., 0.95, 1.0]<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>ì‹¤ì œ ë¶„í¬ì™€ ì¼ì¹˜<br/>ì–‘ìí™” ì˜¤ë¥˜ ì‘ìŒ"]
    end

    INT4 -->|"ê°œì„ "| NF4_Q

    style INT4 fill:#ffe3e3
    style NF4_Q fill:#d3f9d8
```

### Double Quantization

Scale ê°’ë„ ì–‘ìí™”í•˜ì—¬ ë©”ëª¨ë¦¬ë¥¼ ì¶”ê°€ë¡œ ì ˆì•½í•©ë‹ˆë‹¤.

```mermaid
flowchart TB
    subgraph Normal["ì¼ë°˜ ì–‘ìí™”"]
        N1["Weight Group (64ê°œ)"]
        N2["Scale: FP32 (4 bytes)"]
        N3["ë©”ëª¨ë¦¬: 64Ã—0.5 + 4 = 36 bytes"]
    end

    subgraph Double["Double Quantization"]
        D1["Weight Group (64ê°œ)"]
        D2["Scale: FP8 (1 byte)"]
        D3["ë©”ëª¨ë¦¬: 64Ã—0.5 + 1 = 33 bytes"]
        D4["Scale ì €ì¥ 75% ì ˆì•½!"]
    end

    Normal -->|"ê°œì„ "| Double

    style D4 fill:#d3f9d8
```

### QLoRA ì „ì²´ êµ¬ì¡°

```mermaid
flowchart TB
    subgraph Model["ëª¨ë¸ êµ¬ì¡°"]
        Base["Base Model<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>4-bit NF4 ì–‘ìí™”<br/>Frozen â„ï¸"]
        
        LoRA_A["LoRA A<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>FP16<br/>í•™ìŠµ ğŸ”¥"]
        
        LoRA_B["LoRA B<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>FP16<br/>í•™ìŠµ ğŸ”¥"]
    end

    subgraph Forward["ìˆœì „íŒŒ"]
        Input["ì…ë ¥ x"]
        Input --> Base
        Input --> LoRA_A
        Base --> |"Dequant â†’ FP16"| Add["í•©ì‚°"]
        LoRA_A --> LoRA_B --> |"ìŠ¤ì¼€ì¼ë§"| Add
        Add --> Output["ì¶œë ¥"]
    end

    style Base fill:#e7f5ff
    style LoRA_A fill:#d3f9d8
    style LoRA_B fill:#d3f9d8
```

---

## ğŸ“ 2-Stage Training

LLaVAì—ì„œ ì œì•ˆí•œ ë©€í‹°ëª¨ë‹¬ í•™ìŠµ ì „ëµì…ë‹ˆë‹¤.

```mermaid
flowchart TB
    subgraph Stage1["Stage 1: Feature Alignment"]
        S1_Data["ğŸ“Š ë°ì´í„°<br/>Image-Caption ìŒ<br/>(CC3M 595K)"]
        S1_Train["ğŸ¯ í•™ìŠµ ëŒ€ìƒ<br/>Projectorë§Œ"]
        S1_Freeze["â„ï¸ Frozen<br/>Vision Encoder<br/>LLM"]
        S1_Goal["ğŸ’¡ ëª©í‘œ<br/>Vision â†” Language<br/>ê³µê°„ ì •ë ¬"]
        S1_Setting["âš™ï¸ ì„¤ì •<br/>Epochs: 1<br/>LR: 1e-3"]
        
        S1_Data --> S1_Train
        S1_Freeze --> S1_Train
        S1_Train --> S1_Goal
        S1_Goal --> S1_Setting
    end

    subgraph Stage2["Stage 2: Instruction Tuning"]
        S2_Data["ğŸ“Š ë°ì´í„°<br/>Instruction ë°ì´í„°<br/>(AI-Hub ìº¡ì…”ë‹)"]
        S2_Train["ğŸ¯ í•™ìŠµ ëŒ€ìƒ<br/>Projector + LLM (LoRA)"]
        S2_Freeze["â„ï¸ Frozen<br/>Vision Encoder"]
        S2_Goal["ğŸ’¡ ëª©í‘œ<br/>íƒœìŠ¤í¬ íŠ¹í™”<br/>ëŠ¥ë ¥ í•™ìŠµ"]
        S2_Setting["âš™ï¸ ì„¤ì •<br/>Epochs: 3<br/>LR: 2e-5"]
        
        S2_Data --> S2_Train
        S2_Freeze --> S2_Train
        S2_Train --> S2_Goal
        S2_Goal --> S2_Setting
    end

    Stage1 --> Stage2

    style Stage1 fill:#e7f5ff
    style Stage2 fill:#fff3bf
```

### ì™œ 2ë‹¨ê³„ë¡œ ë‚˜ëˆ„ëŠ”ê°€?

```mermaid
flowchart TB
    subgraph Problem["âŒ í•œ ë²ˆì— í•™ìŠµí•˜ë©´"]
        P1["Vision íŠ¹ì§•ì´ LLMì—<br/>ì œëŒ€ë¡œ ì „ë‹¬ ì•ˆë¨"]
        P2["í•™ìŠµ ë¶ˆì•ˆì •"]
        P3["ìˆ˜ë ´ ì–´ë ¤ì›€"]
    end

    subgraph Solution["âœ… 2ë‹¨ê³„ë¡œ ë‚˜ëˆ„ë©´"]
        S1["Stage 1: ë¨¼ì € 'ì–¸ì–´'ë¥¼ ê°€ë¥´ì¹¨<br/>(Vision-Language ì •ë ¬)"]
        S2["Stage 2: ê·¸ ë‹¤ìŒ 'íƒœìŠ¤í¬'ë¥¼ ê°€ë¥´ì¹¨<br/>(ìº¡ì…”ë‹ ëŠ¥ë ¥)"]
    end

    Problem --> |"í•´ê²°"| Solution

    style Problem fill:#ffe3e3
    style Solution fill:#d3f9d8
```

---

## ğŸ¯ ìš°ë¦¬ í”„ë¡œì íŠ¸ ì ìš©

### GPUë³„ ì„¤ì •

```mermaid
flowchart TB
    subgraph T4_Config["ğŸŸ¡ T4 (16GB)"]
        T4_Method["ë°©ë²•: QLoRA (í•„ìˆ˜)"]
        T4_R["r=8, Î±=16"]
        T4_Batch["batch=1, grad_accum=16"]
        T4_Target["target: attentionë§Œ"]
    end

    subgraph L4_Config["ğŸŸ¢ L4 (24GB)"]
        L4_Method["ë°©ë²•: QLoRA"]
        L4_R["r=16, Î±=32"]
        L4_Batch["batch=2, grad_accum=8"]
        L4_Target["target: attentionë§Œ"]
    end

    subgraph A100_Config["ğŸ”µ A100 (40GB)"]
        A100_Method["ë°©ë²•: LoRA ë˜ëŠ” QLoRA"]
        A100_R["r=32, Î±=64"]
        A100_Batch["batch=4, grad_accum=4"]
        A100_Target["target: attention + MLP"]
    end

    subgraph H100_Config["ğŸŸ£ H100 (80GB)"]
        H100_Method["ë°©ë²•: LoRA"]
        H100_R["r=64, Î±=128"]
        H100_Batch["batch=8, grad_accum=2"]
        H100_Target["target: attention + MLP"]
    end

    style T4_Config fill:#fff3bf
    style L4_Config fill:#d3f9d8
    style A100_Config fill:#d0ebff
    style H100_Config fill:#e5dbff
```

### ì½”ë“œ ì˜ˆì‹œ

```python
from transformers import BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# 1. 4-bit ì–‘ìí™” ì„¤ì •
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",           # NF4 ì‚¬ìš©
    bnb_4bit_use_double_quant=True,      # Double Quantization
)

# 2. ëª¨ë¸ ë¡œë“œ
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto",
)

# 3. LoRA ì¤€ë¹„
model = prepare_model_for_kbit_training(model)

# 4. LoRA ì„¤ì •
lora_config = LoraConfig(
    r=16,                                 # Rank
    lora_alpha=32,                        # Scaling
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

# 5. LoRA ì ìš©
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# ì¶œë ¥: trainable params: 16,777,216 || all params: 7,000,000,000 || trainable%: 0.24%
```

---

## ğŸ“š ë…¼ë¬¸ ëª©ë¡

| íŒŒì¼ | ë…¼ë¬¸ | í•µì‹¬ í¬ì¸íŠ¸ | ì¤‘ìš”ë„ |
|------|------|------------|--------|
| [lora.md](lora.md) | LoRA (2021) | PEFTì˜ ê¸°ì´ˆ | â­â­â­â­ |
| [qlora.md](qlora.md) | QLoRA (2023) | **T4/L4 í•„ìˆ˜** | â­â­â­â­â­ |
| [dora.md](dora.md) | DoRA (2024) | LoRA ê°œì„  | â­â­â­ |
| [llava_2stage.md](llava_2stage.md) | 2-Stage (2023) | **ë©€í‹°ëª¨ë‹¬ í•™ìŠµ ì „ëµ** | â­â­â­â­â­ |
