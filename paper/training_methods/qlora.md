# QLoRA: Efficient Finetuning of Quantized LLMs

> â­ **T4/L4ì—ì„œ í•„ìˆ˜**: 4-bit ì–‘ìí™” + LoRAë¡œ 7B ëª¨ë¸ì„ 6GBì—ì„œ í•™ìŠµ

- **ì €ì**: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer
- **ê¸°ê´€**: University of Washington
- **ì—°ë„**: 2023
- **ë§í¬**: [arXiv:2305.14314](https://arxiv.org/abs/2305.14314)

---

## í•µì‹¬ ê¸°ì—¬

1. **NF4 (NormalFloat 4-bit)**: ì •ê·œë¶„í¬ ê¸°ë°˜ ì–‘ìí™”ë¡œ í’ˆì§ˆ ì†ì‹¤ ìµœì†Œí™”
2. **Double Quantization**: ì–‘ìí™” ìƒìˆ˜ë„ ì–‘ìí™”í•˜ì—¬ ë©”ëª¨ë¦¬ ì¶”ê°€ ì ˆì•½
3. **Paged Optimizers**: GPU OOM ì‹œ ìë™ ìŠ¤ì™‘ìœ¼ë¡œ ì•ˆì •ì  í•™ìŠµ
4. **ê·¹ì ì¸ ë©”ëª¨ë¦¬ ì ˆì•½**: 112GB â†’ 6GB (95% ì ˆì•½!)

---

## ë©”ëª¨ë¦¬ ë¹„êµ

![Training Memory Comparison](../../../model_viz/outputs/training_memory.png)

### ë©”ëª¨ë¦¬ êµ¬ì„± ìš”ì†Œ ìƒì„¸

```mermaid
flowchart TB
    subgraph FullFT["Full Fine-tuning: 112GB"]
        F1["ëª¨ë¸ (FP16): 14GB"]
        F2["Gradients: 14GB"]
        F3["Optimizer (Adam): 56GB<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>momentum + variance<br/>= 2 Ã— FP32 states"]
        F4["Activations: 28GB"]
    end

    subgraph QLoRA_Mem["QLoRA: 6GB"]
        Q1["ëª¨ë¸ (4-bit): 3.5GB"]
        Q2["LoRA weights (FP16): 32MB"]
        Q3["Optimizer (LoRAë§Œ): 128MB"]
        Q4["Activations: 2GB"]
    end

    style F3 fill:#ffe3e3
    style QLoRA_Mem fill:#d3f9d8
```

---

## NF4 (NormalFloat 4-bit)

### ì™œ NF4ì¸ê°€?

| ë°©ì‹ | ë¶„í¬ ê°€ì • | íŠ¹ì§• |
|------|----------|------|
| INT4 | ê· ì¼ ë¶„í¬ | 16ê°œ ê· ë“± ê°„ê²© ê°’ |
| **NF4** | **ì •ê·œ ë¶„í¬** | 0 ê·¼ì²˜ì— ë” ì´˜ì´˜í•œ ê°’ â†’ ì‹¤ì œ ê°€ì¤‘ì¹˜ ë¶„í¬ì™€ ì¼ì¹˜ |

### Double Quantization

- **ì¼ë°˜ ì–‘ìí™”**: Weight block (64ê°œ) + Scale FP32 (4B) = 36 bytes
- **Double Quantization**: Weight block (64ê°œ) + Scale FP8 (1B) = 33 bytes
- **ì ˆì•½**: 7B ëª¨ë¸ ê¸°ì¤€ ì•½ 328MB (0.3GB)

---

## QLoRA ì „ì²´ êµ¬ì¡°

![LoRA Architecture](../../../model_viz/outputs/lora_architecture.png)

**QLoRA = 4-bit Base Model (Frozen) + FP16 LoRA Adapters (Trainable)**

- **ìˆœì „íŒŒ**: Dequant(4-bit â†’ FP16) + LoRA ì¶œë ¥ í•©ì‚°
- **ì—­ì „íŒŒ**: GradientëŠ” LoRAë§Œ, Base modelì€ frozen

---

## ìš°ë¦¬ í”„ë¡œì íŠ¸ ì ìš©

### í•„ìˆ˜ ì„¤ì • (T4/L4)

```python
from transformers import BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,                    # 4-bit ì–‘ìí™” í™œì„±í™”
    bnb_4bit_compute_dtype=torch.float16, # ì—°ì‚°ì€ FP16
    bnb_4bit_quant_type="nf4",            # NF4 ì‚¬ìš© (í•µì‹¬!)
    bnb_4bit_use_double_quant=True,       # Double Quantization
)
```

### GPUë³„ ì„¤ì •

| GPU | ë°©ì‹ | r | alpha | batch | target |
|-----|------|---|-------|-------|--------|
| T4 (16GB) | QLoRA í•„ìˆ˜ | 8 | 16 | 1 | attention |
| L4 (24GB) | QLoRA ê¶Œì¥ | 16 | 32 | 2 | attention |
| A100 (40GB) | QLoRA/LoRA | 32 | 64 | 4 | attn+MLP |
| H100 (80GB) | LoRA ê°€ëŠ¥ | 64 | 128 | 8 | ì „ì²´ |

### ì „ì²´ ì½”ë“œ ì˜ˆì‹œ

```python
from transformers import (
    AutoModelForCausalLM,
    BitsAndBytesConfig,
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
)

# 1. 4-bit ì–‘ìí™” ì„¤ì •
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
)

# 2. ëª¨ë¸ ë¡œë“œ (4-bit)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto",
)

# 3. k-bit í•™ìŠµ ì¤€ë¹„
model = prepare_model_for_kbit_training(model)

# 4. LoRA ì„¤ì •
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

# 5. PEFT ëª¨ë¸ ìƒì„±
model = get_peft_model(model, lora_config)

# í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„° í™•ì¸
model.print_trainable_parameters()
# ì¶œë ¥ ì˜ˆ: trainable params: 16,777,216 || all params: 7,000,000,000 || trainable%: 0.24%
```

---

## âš ï¸ êµ¬í˜„ ì‹œ ì£¼ì˜ì 

```mermaid
flowchart TB
    subgraph Cautions["ì£¼ì˜ì‚¬í•­"]
        C1["1ï¸âƒ£ compute_dtype ì„¤ì •<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>T4/L4: float16<br/>A100/H100: bfloat16 ê°€ëŠ¥"]
        
        C2["2ï¸âƒ£ DDP ë¹„í˜¸í™˜<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>device_map='auto'ì™€<br/>DDP ë™ì‹œ ì‚¬ìš© ë¶ˆê°€"]
        
        C3["3ï¸âƒ£ Gradient Checkpointing<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>ì¶”ê°€ ë©”ëª¨ë¦¬ ì ˆì•½ ê°€ëŠ¥<br/>ì†ë„ëŠ” ì•½ê°„ ì €í•˜"]
        
        C4["4ï¸âƒ£ Learning Rate<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>ì¼ë°˜ LoRAë³´ë‹¤ ë†’ê²Œ<br/>2e-4 ì •ë„ ê¶Œì¥"]
    end

    style C1 fill:#e7f5ff
    style C2 fill:#ffe3e3
    style C3 fill:#fff3bf
    style C4 fill:#d3f9d8
```

---

## ğŸ“ˆ ì„±ëŠ¥ (Guanaco ë²¤ì¹˜ë§ˆí¬)

| Model | Method | Params | Memory | MMLU | HellaSwag |
|-------|--------|--------|--------|------|-----------|
| LLaMA-7B | Full FT | 7B | 112GB | 35.1 | 76.2 |
| LLaMA-7B | LoRA | 16M | 56GB | 36.2 | 77.1 |
| LLaMA-7B | **QLoRA** | 16M | **6GB** | **36.5** | **77.3** |

> ğŸ’¡ 4-bit ì–‘ìí™”ì—ë„ ì„±ëŠ¥ ì €í•˜ ê±°ì˜ ì—†ìŒ!

---

## ğŸ”— ê´€ë ¨ ë¦¬ì†ŒìŠ¤

- **bitsandbytes**: `pip install bitsandbytes`
- **PEFT**: `pip install peft`
- **GitHub**: [artidoro/qlora](https://github.com/artidoro/qlora)
- **Paper**: [arXiv:2305.14314](https://arxiv.org/abs/2305.14314)

---

## ğŸ“š ì¸ìš©

```bibtex
@inproceedings{dettmers2023qlora,
  title={QLoRA: Efficient Finetuning of Quantized LLMs},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  booktitle={NeurIPS},
  year={2023}
}
```
