# LLaVA-NeXT-Video: A Strong Zero-shot Video Understanding Model

> â­ **ìš°ë¦¬ í”„ë¡œì íŠ¸ì˜ ê¸°ë³¸ ëª¨ë¸**: ì´ë¯¸ì§€ í•™ìŠµë§Œìœ¼ë¡œ ë¹„ë””ì˜¤ë¥¼ ì´í•´í•˜ëŠ” Zero-shot ëŠ¥ë ¥

- **ì €ì**: Haotian Liu et al.
- **ê¸°ê´€**: ByteDance, University of Wisconsin-Madison
- **ì—°ë„**: 2024
- **ë§í¬**: [Blog](https://llava-vl.github.io/blog/2024-04-30-llava-next-video/)

---

## ğŸ’¡ í•µì‹¬ ê¸°ì—¬

```mermaid
flowchart TB
    subgraph Contributions["LLaVA-NeXT-Videoì˜ í•µì‹¬ ê¸°ì—¬"]
        C1["1ï¸âƒ£ Zero-shot Video<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>ì´ë¯¸ì§€ë§Œìœ¼ë¡œ í•™ìŠµí•´ë„<br/>ë¹„ë””ì˜¤ ì´í•´ ê°€ëŠ¥!"]
        
        C2["2ï¸âƒ£ AnyRes for Video<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>í”„ë ˆì„ë³„ ê³ í•´ìƒë„ ì²˜ë¦¬<br/>+ Spatial Pooling"]
        
        C3["3ï¸âƒ£ DPO Training<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Direct Preference Opt.<br/>Hallucination ê°ì†Œ"]
        
        C4["4ï¸âƒ£ íš¨ìœ¨ì  ì¸ì½”ë”©<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>í”„ë ˆì„ë³„ ë…ë¦½ ì¸ì½”ë”©<br/>ë©”ëª¨ë¦¬ íš¨ìœ¨ì "]
    end

    style C1 fill:#d3f9d8
    style C2 fill:#fff3bf
    style C3 fill:#e7f5ff
    style C4 fill:#e5dbff
```

---

## ì•„í‚¤í…ì²˜

![VLM Architecture](../../../model_viz/outputs/vlm_architecture.png)

![Video Frame Processing](../../../model_viz/outputs/video_frame_processing.png)

### ì „ì²´ êµ¬ì¡°

```mermaid
flowchart TB
    subgraph Input["ğŸ“¥ ë¹„ë””ì˜¤ ì…ë ¥"]
        Video["ğŸ¬ ë¹„ë””ì˜¤<br/>T frames ì¶”ì¶œ"]
    end

    subgraph Sampling["ğŸ“Š í”„ë ˆì„ ìƒ˜í”Œë§"]
        Sample["Uniform Sampling<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>ê¸°ë³¸ 8 frames<br/>(ì¡°ì ˆ ê°€ëŠ¥: 4~32)"]
    end

    subgraph PerFrame["ğŸ‘ï¸ í”„ë ˆì„ë³„ ì¸ì½”ë”©"]
        F1["Frame 1"] --> E1["CLIP<br/>576 tokens"]
        F2["Frame 2"] --> E2["CLIP<br/>576 tokens"]
        F3["..."] --> E3["..."]
        FT["Frame T"] --> ET["CLIP<br/>576 tokens"]
    end

    subgraph Pooling["ğŸ”„ Spatial Pooling"]
        Pool["2Ã—2 Average Pool<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>576 â†’ 144 tokens/frame<br/>ì´: T Ã— 144 tokens"]
    end

    subgraph Projector["ğŸ”— Projector"]
        Proj["Linear Layer<br/>Visual â†’ Language"]
    end

    subgraph LLM["ğŸ§  LLM"]
        Model["Vicuna-7B<br/>+ Text Prompt"]
    end

    subgraph Output["ğŸ“¤ ì¶œë ¥"]
        Caption["ìƒì„±ëœ ìº¡ì…˜"]
    end

    Video --> Sample --> PerFrame
    E1 --> Pool
    E2 --> Pool
    ET --> Pool
    Pool --> Proj --> LLM --> Caption

    style Pool fill:#fff3bf
    style LLM fill:#d3f9d8
```

### í† í° ìˆ˜ ê³„ì‚°

```mermaid
flowchart LR
    subgraph Calculation["í† í° ìˆ˜ ê³„ì‚°"]
        C1["í”„ë ˆì„ë‹¹ ì›ë³¸<br/>336Ã·14 = 24<br/>24Ã—24 = 576 tokens"]
        
        C2["Spatial Pooling í›„<br/>2Ã—2 average<br/>24Ã·2 = 12<br/>12Ã—12 = 144 tokens"]
        
        C3["8 frames ê¸°ì¤€<br/>144 Ã— 8 = 1,152 tokens<br/>+ Text tokens"]
    end

    C1 --> C2 --> C3

    style C3 fill:#d3f9d8
```

---

## ğŸ“Š Zero-shot Video Understanding

### ì™œ ê°€ëŠ¥í•œê°€?

```mermaid
flowchart TB
    subgraph Why["Zero-shotì´ ê°€ëŠ¥í•œ ì´ìœ "]
        W1["ì´ë¯¸ì§€ = ë¹„ë””ì˜¤ì˜ í•œ í”„ë ˆì„<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>ì´ë¯¸ì§€ ì´í•´ ëŠ¥ë ¥ì´<br/>ë¹„ë””ì˜¤ë¡œ ì „ì´ë¨"]
        
        W2["í”„ë ˆì„ë³„ ë…ë¦½ ì¸ì½”ë”©<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>ì‹œê°„ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬í•˜ë©´<br/>ë¹„ë””ì˜¤ ë§¥ë½ ì´í•´"]
        
        W3["LLMì˜ ì¼ë°˜í™” ëŠ¥ë ¥<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>ì—¬ëŸ¬ í”„ë ˆì„ ì •ë³´ë¥¼<br/>í†µí•©í•˜ì—¬ ì´í•´"]
    end

    W1 --> W2 --> W3

    subgraph Result["ê²°ê³¼"]
        R["ë¹„ë””ì˜¤ í•™ìŠµ ì—†ì´ë„<br/>ë¹„ë””ì˜¤ QA, ìº¡ì…”ë‹ ê°€ëŠ¥!"]
    end

    W3 --> Result

    style Result fill:#d3f9d8
```

### í•œê³„ì 

```mermaid
flowchart TB
    subgraph Limitations["í•œê³„ì "]
        L1["âŒ Temporal ê´€ê³„ ì•½í•¨<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>í”„ë ˆì„ë³„ ë…ë¦½ ì¸ì½”ë”©<br/>â†’ ì‹œê°„ì  ê´€ê³„ ì•”ë¬µì "]
        
        L2["âŒ ê¸´ ë¹„ë””ì˜¤ ì–´ë ¤ì›€<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>í”„ë ˆì„ ìˆ˜ ì œí•œ<br/>ì •ë³´ ì†ì‹¤ ê°€ëŠ¥"]
        
        L3["âŒ ë¹ ë¥¸ ë™ì‘ ìº¡ì²˜ ì–´ë ¤ì›€<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Uniform sampling<br/>â†’ ì¤‘ìš” ìˆœê°„ ë†“ì¹  ìˆ˜ ìˆìŒ"]
    end

    style L1 fill:#ffe3e3
    style L2 fill:#ffe3e3
    style L3 fill:#ffe3e3
```

---

## ğŸ“ í•™ìŠµ ì „ëµ

### Stage 1: Image Pre-training

```mermaid
flowchart TB
    subgraph Stage1["Stage 1: ì´ë¯¸ì§€ í•™ìŠµ"]
        D1["LLaVA-NeXT ì´ë¯¸ì§€ ëª¨ë¸<br/>ê·¸ëŒ€ë¡œ ì‚¬ìš©"]
        N1["ë¹„ë””ì˜¤ ë°ì´í„° ì—†ì´ í•™ìŠµ!"]
    end

    style Stage1 fill:#e7f5ff
```

### Stage 2: Video Fine-tuning (Optional)

```mermaid
flowchart TB
    subgraph Stage2["Stage 2: ë¹„ë””ì˜¤ Fine-tuning (ì„ íƒì )"]
        subgraph Data["ğŸ“Š ë°ì´í„°"]
            D2["Video-ChatGPT<br/>ActivityNet-QA<br/>NExT-QA"]
        end

        subgraph Method["ğŸ¯ ë°©ë²•"]
            M1["Video Instruction Tuning"]
            M2["DPO (Preference Learning)"]
        end

        subgraph Goal["ğŸ’¡ ëª©í‘œ"]
            G["ë¹„ë””ì˜¤ íŠ¹í™” ëŠ¥ë ¥ ê°•í™”<br/>Hallucination ê°ì†Œ"]
        end
    end

    Data --> Method --> Goal

    style Method fill:#fff3bf
```

### DPO (Direct Preference Optimization)

```mermaid
flowchart TB
    subgraph DPO["DPO Training"]
        Input["ë¹„ë””ì˜¤ + ì§ˆë¬¸"]
        
        subgraph Responses["ì‘ë‹µ ìŒ"]
            Good["âœ… ì„ í˜¸ ì‘ë‹µ<br/>(ì •í™•í•œ ì„¤ëª…)"]
            Bad["âŒ ë¹„ì„ í˜¸ ì‘ë‹µ<br/>(Hallucination)"]
        end

        subgraph Training["í•™ìŠµ"]
            T["ì„ í˜¸ ì‘ë‹µ í™•ë¥  â†‘<br/>ë¹„ì„ í˜¸ ì‘ë‹µ í™•ë¥  â†“"]
        end

        Input --> Responses --> Training
    end

    subgraph Effect["íš¨ê³¼"]
        E["Hallucination ê°ì†Œ<br/>ë” ì •í™•í•œ ìº¡ì…˜ ìƒì„±"]
    end

    DPO --> Effect

    style Good fill:#d3f9d8
    style Bad fill:#ffe3e3
    style Effect fill:#d3f9d8
```

---

## ğŸ¯ ìš°ë¦¬ í”„ë¡œì íŠ¸ ì ìš©

### í”„ë ˆì„ ìˆ˜ ì„¤ì •

```mermaid
flowchart TB
    subgraph FrameGuide["GPUë³„ í”„ë ˆì„ ìˆ˜ ê¶Œì¥"]
        T4["ğŸŸ¡ T4 (16GB)<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>frames: 4<br/>tokens: 576<br/>ë©”ëª¨ë¦¬ ì œì•½"]
        
        L4["ğŸŸ¢ L4 (24GB)<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>frames: 8 (ê¸°ë³¸)<br/>tokens: 1,152<br/>ê¶Œì¥ ì„¤ì •"]
        
        A100["ğŸ”µ A100 (40GB)<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>frames: 16<br/>tokens: 2,304<br/>ê³ í’ˆì§ˆ"]
        
        H100["ğŸŸ£ H100 (80GB)<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>frames: 32<br/>tokens: 4,608<br/>ìµœëŒ€ í’ˆì§ˆ"]
    end

    style T4 fill:#fff3bf
    style L4 fill:#d3f9d8
    style A100 fill:#d0ebff
    style H100 fill:#e5dbff
```

### ì½”ë“œ ì˜ˆì‹œ

```python
from transformers import LlavaNextVideoForConditionalGeneration, LlavaNextVideoProcessor

# ëª¨ë¸ ë¡œë“œ
model_id = "llava-hf/LLaVA-NeXT-Video-7B-hf"
processor = LlavaNextVideoProcessor.from_pretrained(model_id)
model = LlavaNextVideoForConditionalGeneration.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map="auto",
)

# ë¹„ë””ì˜¤ í”„ë ˆì„ ì¤€ë¹„ (PIL Images ë¦¬ìŠ¤íŠ¸)
frames = extract_frames(video_path, num_frames=8)

# í”„ë¡¬í”„íŠ¸
prompt = "USER: <video>ì´ ì˜ìƒì„ í•œêµ­ì–´ë¡œ ìƒì„¸íˆ ë¬˜ì‚¬í•´ì£¼ì„¸ìš”. ASSISTANT:"

# ì¶”ë¡ 
inputs = processor(text=prompt, videos=frames, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=256)
caption = processor.decode(outputs[0], skip_special_tokens=True)
```

### Fine-tuning ì „ëµ

```mermaid
flowchart TB
    subgraph Strategy["í•œêµ­ì–´ ë¹„ë””ì˜¤ ìº¡ì…”ë‹ Fine-tuning"]
        S1["Stage 1 (ì„ íƒì )<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>í•œêµ­ì–´ ì´ë¯¸ì§€-ìº¡ì…˜ìœ¼ë¡œ<br/>Projector ì •ë ¬"]
        
        S2["Stage 2 (í•„ìˆ˜)<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>AI-Hub ë¹„ë””ì˜¤ ë°ì´í„°ë¡œ<br/>QLoRA Fine-tuning"]
        
        S3["í”„ë¡¬í”„íŠ¸<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>USER: <video><br/>ì´ ì˜ìƒì„ í•œêµ­ì–´ë¡œ<br/>ìƒì„¸íˆ ë¬˜ì‚¬í•´ì£¼ì„¸ìš”.<br/>ASSISTANT:"]
    end

    S1 --> S2 --> S3

    style S2 fill:#d3f9d8
```

---

## ğŸ“ˆ ì„±ëŠ¥ (Zero-shot)

| Benchmark | LLaVA-NeXT-Video-7B | LLaVA-NeXT-Video-7B-DPO |
|-----------|---------------------|------------------------|
| **ActivityNet-QA** | 53.5 | **56.2** |
| **MSVD-QA** | 67.8 | **70.1** |
| **MSRVTT-QA** | 53.2 | **55.8** |
| **TGIF-QA** | 67.1 | **69.3** |

---

## âš ï¸ êµ¬í˜„ ì‹œ ì£¼ì˜ì 

```mermaid
flowchart TB
    subgraph Cautions["ì£¼ì˜ì‚¬í•­"]
        C1["1ï¸âƒ£ ë©”ëª¨ë¦¬ ê´€ë¦¬<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>8 frames Ã— 144 = 1,152 tokens<br/>4-bit ì–‘ìí™” ê¶Œì¥ (T4/L4)"]
        
        C2["2ï¸âƒ£ í”„ë ˆì„ ìƒ˜í”Œë§<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Uniformì´ ê¸°ë³¸<br/>ì¥ë©´ ë³€í™” ê¸°ë°˜ adaptive ê³ ë ¤"]
        
        C3["3ï¸âƒ£ Spatial Pooling<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>ê¸°ë³¸ 2Ã—2<br/>ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ 3Ã—3 ê°€ëŠ¥"]
        
        C4["4ï¸âƒ£ í”„ë¡¬í”„íŠ¸ í˜•ì‹<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>ë°˜ë“œì‹œ <video> í† í° í¬í•¨<br/>ASSISTANT: ë¡œ ëë‚´ê¸°"]
    end

    style C1 fill:#ffe3e3
    style C2 fill:#fff3bf
    style C3 fill:#e7f5ff
    style C4 fill:#d3f9d8
```

---

## ğŸ”— ê´€ë ¨ ë¦¬ì†ŒìŠ¤

- **Hugging Face**: 
  - `llava-hf/LLaVA-NeXT-Video-7B-hf`
  - `llava-hf/LLaVA-NeXT-Video-7B-DPO-hf` (DPO ì ìš©)
  - `llava-hf/LLaVA-NeXT-Video-34B-hf` (ëŒ€í˜• ëª¨ë¸)
- **GitHub**: [LLaVA-VL/LLaVA-NeXT](https://github.com/LLaVA-VL/LLaVA-NeXT)
- **Blog**: [llava-vl.github.io](https://llava-vl.github.io/blog/2024-04-30-llava-next-video/)

---

## ğŸ“š ì¸ìš©

```bibtex
@misc{liu2024llavanext,
  title={LLaVA-NeXT: A Strong Zero-shot Video Understanding Model},
  author={Liu, Haotian and others},
  year={2024},
  howpublished={\url{https://llava-vl.github.io/blog/2024-04-30-llava-next-video/}}
}
```
