# Qwen3: A Family of Large Language Models

> â­ **í•œêµ­ì–´ ìµœê³  ì„±ëŠ¥**: MoE ì§€ì›, 128K ì»¨í…ìŠ¤íŠ¸, Apache-2.0 ë¼ì´ì„ ìŠ¤

- **ê¸°ê´€**: Alibaba Cloud
- **ì—°ë„**: 2025
- **ë§í¬**: [GitHub](https://github.com/QwenLM/Qwen3)

---

## í•µì‹¬ ê¸°ì—¬

1. **ë‹¤ì–‘í•œ í¬ê¸°**: 0.6B ~ 235B, ëª¨ë“  GPUì—ì„œ ì‚¬ìš© ê°€ëŠ¥
2. **MoE ì§€ì›**: 30B-A3B, 235B-A22B íš¨ìœ¨ì ì¸ ëŒ€í˜• ëª¨ë¸
3. **128K ì»¨í…ìŠ¤íŠ¸**: ê¸´ ë¹„ë””ì˜¤ ì„¤ëª…ì— ìœ ë¦¬ (RoPE í™•ì¥)
4. **í•œêµ­ì–´ ìµœê³  ì„±ëŠ¥**: MMLU-Ko 72.3% (Vicuna ëŒ€ë¹„ 2ë°°)
5. **Apache-2.0 ë¼ì´ì„ ìŠ¤**: ìƒì—…ì  ì‚¬ìš© ììœ 

---

## ëª¨ë¸ ë¼ì¸ì—…

### Dense Models

| ëª¨ë¸ | íŒŒë¼ë¯¸í„° | ê¶Œì¥ GPU | 4-bit ë©”ëª¨ë¦¬ |
|------|---------|---------|-------------|
| Qwen3-0.6B | 0.6B | CPU | ~0.4GB |
| Qwen3-1.7B | 1.7B | ëª¨ë“  GPU | ~1GB |
| **Qwen3-4B** | 4B | **T4** | ~3GB |
| **Qwen3-8B** | 8B | **L4** | ~5GB |
| **Qwen3-14B** | 14B | **A100** | ~8GB |
| Qwen3-32B | 32B | H100 | ~18GB |

### MoE Models

| ëª¨ë¸ | ì´ íŒŒë¼ë¯¸í„° | í™œì„± íŒŒë¼ë¯¸í„° | íŠ¹ì§• |
|------|-----------|-------------|------|
| Qwen3-30B-A3B | 30B | 3B (10%) | Dense 3B ì—°ì‚°, Dense 30B ì„±ëŠ¥ |
| Qwen3-235B-A22B | 235B | 22B | ìµœê³  ì„±ëŠ¥, ë‹¤ì¤‘ GPU í•„ìš” |

---

## ì•„í‚¤í…ì²˜ ìƒì„¸

### Decoder Layer êµ¬ì¡°

![Decoder Layer](../../../model_viz/outputs/decoder_layer.png)

### GQA (Grouped Query Attention)

![GQA Comparison](../../../model_viz/outputs/gqa_comparison.png)

> Qwen3ëŠ” GQAë¡œ KV Cacheë¥¼ 4ë°° ì ˆì•½

| ë°©ì‹ | Q Heads | KV Heads | KV Cache |
|------|---------|----------|----------|
| MHA | 32 | 32 | 32 Ã— dim |
| **GQA** | 32 | 8 | **8 Ã— dim (4ë°° ê°ì†Œ)** |

### MoE (Mixture of Experts)

![MoE Architecture](../../../model_viz/outputs/moe_architecture.png)

> Qwen3-30B-A3B: ì´ 30B íŒŒë¼ë¯¸í„° ì¤‘ 3Bë§Œ í™œì„±í™”

### ì–´íœ˜ì§‘ (Vocabulary)

```mermaid
flowchart LR
    subgraph Vocab["ì–´íœ˜ì§‘ ë¹„êµ"]
        LLaMA["LLaMA: 32,000 tokens<br/>ì˜ì–´ ì¤‘ì‹¬"]
        Qwen["Qwen3: 151,936 tokens<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>â€¢ í•œêµ­ì–´ í† í° í’ë¶€<br/>â€¢ ì¤‘êµ­ì–´ í† í° í’ë¶€<br/>â€¢ ë‹¤êµ­ì–´ íš¨ìœ¨ì "]
    end

    subgraph Effect["íš¨ê³¼"]
        E["í•œêµ­ì–´ í…ìŠ¤íŠ¸ê°€<br/>ë” ì ì€ í† í°ìœ¼ë¡œ í‘œí˜„<br/>â†’ íš¨ìœ¨ì  + ì •í™•"]
    end

    Qwen --> Effect

    style Qwen fill:#d3f9d8
```

---

## ğŸ“Š í•œêµ­ì–´ ì„±ëŠ¥ ë¹„êµ

```mermaid
xychart-beta
    title "í•œêµ­ì–´ MMLU ì„±ëŠ¥ ë¹„êµ"
    x-axis ["Vicuna-7B", "LLaMA3-8B", "Qwen-7B", "Qwen2-7B", "Qwen3-8B", "Qwen3-14B"]
    y-axis "ì ìˆ˜ (%)" 30 --> 80
    bar [38.2, 48.7, 52.1, 62.1, 68.5, 72.3]
```

### ìƒì„¸ ë¹„êµí‘œ

| ëª¨ë¸ | íŒŒë¼ë¯¸í„° | ì»¨í…ìŠ¤íŠ¸ | í•œêµ­ì–´ MMLU | íŠ¹ì§• |
|------|----------|----------|------------|------|
| Vicuna-7B | 7B | 4K | 38.2% | LLaVA ê¸°ë³¸ |
| Qwen-7B | 7B | 32K | 52.1% | 1ì„¸ëŒ€ |
| Qwen2-7B | 7B | 128K | 62.1% | GQA ë„ì… |
| **Qwen3-8B** | 8B | 128K | **68.5%** | â­ L4 ê¶Œì¥ |
| **Qwen3-14B** | 14B | 128K | **72.3%** | â­ A100 ê¶Œì¥ |

---

## ğŸ¯ ìš°ë¦¬ í”„ë¡œì íŠ¸ ì ìš©

### GPUë³„ ê¶Œì¥ ëª¨ë¸

```mermaid
flowchart TB
    subgraph GPU_Choice["GPUë³„ Qwen3 ì„ íƒ"]
        T4["ğŸŸ¡ T4 (16GB)<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Qwen3-4B-Instruct<br/>4-bit: ~3GB<br/>í•œêµ­ì–´: ~60%"]
        
        L4["ğŸŸ¢ L4 (24GB)<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Qwen3-8B-Instruct<br/>4-bit: ~5GB<br/>í•œêµ­ì–´: 68.5%"]
        
        A100["ğŸ”µ A100 (40GB)<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Qwen3-14B-Instruct<br/>4-bit: ~8GB<br/>í•œêµ­ì–´: 72.3%"]
        
        H100["ğŸŸ£ H100 (80GB)<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Qwen3-32B-Instruct<br/>ë˜ëŠ” 30B-A3B (MoE)<br/>í•œêµ­ì–´: ~75%"]
    end

    style T4 fill:#fff3bf
    style L4 fill:#d3f9d8
    style A100 fill:#d0ebff
    style H100 fill:#e5dbff
```

### í”„ë¡¬í”„íŠ¸ í˜•ì‹

```python
# Qwen3 Chat Template
messages = [
    {"role": "system", "content": "ë‹¹ì‹ ì€ ë¹„ë””ì˜¤ ìº¡ì…”ë‹ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
    {"role": "user", "content": "<video>ì´ ì˜ìƒì„ í•œêµ­ì–´ë¡œ ìƒì„¸íˆ ë¬˜ì‚¬í•´ì£¼ì„¸ìš”."}
]

# apply_chat_template ì‚¬ìš©
text = tokenizer.apply_chat_template(messages, tokenize=False)

# ê²°ê³¼:
# <|im_start|>system
# ë‹¹ì‹ ì€ ë¹„ë””ì˜¤ ìº¡ì…”ë‹ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.<|im_end|>
# <|im_start|>user
# <video>ì´ ì˜ìƒì„ í•œêµ­ì–´ë¡œ ìƒì„¸íˆ ë¬˜ì‚¬í•´ì£¼ì„¸ìš”.<|im_end|>
# <|im_start|>assistant
```

### LLM êµì²´ ì‹œ í•„ìš” ì‘ì—…

```mermaid
flowchart TB
    subgraph Change["Vicuna â†’ Qwen3 êµì²´"]
        direction TB
        C1["1ï¸âƒ£ Projector ì¬í•™ìŠµ<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Vision â†’ Qwen3 ì •ë ¬<br/>ì°¨ì›ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ"]
        
        C2["2ï¸âƒ£ í† í¬ë‚˜ì´ì € ë³€ê²½<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>ì–´íœ˜ í¬ê¸°: 32K â†’ 152K<br/>special tokens ë‹¤ë¦„"]
        
        C3["3ï¸âƒ£ í”„ë¡¬í”„íŠ¸ ìˆ˜ì •<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>USER/ASSISTANT â†’<br/>im_start/im_end"]
        
        C4["4ï¸âƒ£ LoRA ì„¤ì • ì¡°ì •<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>target_modules í™•ì¸<br/>ëª¨ë¸ êµ¬ì¡°ì— ë§ê²Œ"]
    end

    C1 --> C2 --> C3 --> C4

    style C1 fill:#ffe3e3
    style C2 fill:#fff3bf
    style C3 fill:#e7f5ff
    style C4 fill:#d3f9d8
```

### ì½”ë“œ ì˜ˆì‹œ

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

# 4-bit ì–‘ìí™” ì„¤ì •
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
)

# ëª¨ë¸ ë¡œë“œ
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen3-8B-Instruct",
    quantization_config=bnb_config,
    device_map="auto",
)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-8B-Instruct")

# ëŒ€í™” í˜•ì‹ ì ìš©
messages = [
    {"role": "system", "content": "ë‹¹ì‹ ì€ ë¹„ë””ì˜¤ ìº¡ì…”ë‹ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
    {"role": "user", "content": "í•œêµ­ì˜ ì „í†µ ê±´ì¶•ë¬¼ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”."}
]
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

# ìƒì„±
inputs = tokenizer(text, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=256)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
```

---

## âš ï¸ êµ¬í˜„ ì‹œ ì£¼ì˜ì 

```mermaid
flowchart TB
    subgraph Cautions["ì£¼ì˜ì‚¬í•­"]
        C1["1ï¸âƒ£ í† í¬ë‚˜ì´ì €<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Qwen ì „ìš© í† í¬ë‚˜ì´ì €<br/>BPE ê¸°ë°˜"]
        
        C2["2ï¸âƒ£ Special Tokens<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/><|im_start|>, <|im_end|><br/>ì •í™•íˆ ì²˜ë¦¬"]
        
        C3["3ï¸âƒ£ LoRA Targets<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>q_proj, k_proj, v_proj,<br/>o_proj, gate_proj,<br/>up_proj, down_proj"]
        
        C4["4ï¸âƒ£ bfloat16 ì§€ì›<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>A100/H100ì—ì„œ<br/>bfloat16 ì‚¬ìš© ê¶Œì¥"]
    end

    style C1 fill:#e7f5ff
    style C2 fill:#ffe3e3
    style C3 fill:#fff3bf
    style C4 fill:#d3f9d8
```

---

## ğŸ”— ê´€ë ¨ ë¦¬ì†ŒìŠ¤

- **Hugging Face**:
  - `Qwen/Qwen3-4B-Instruct`
  - `Qwen/Qwen3-8B-Instruct`
  - `Qwen/Qwen3-14B-Instruct`
  - `Qwen/Qwen3-32B-Instruct`
  - `Qwen/Qwen3-30B-A3B` (MoE)
- **GitHub**: [QwenLM/Qwen3](https://github.com/QwenLM/Qwen3)
- **ë¼ì´ì„ ìŠ¤**: Apache-2.0 (ìƒì—…ì  ì‚¬ìš© ììœ )

---

## ğŸ“š ì¸ìš©

```bibtex
@misc{qwen3,
  title={Qwen3 Technical Report},
  author={Alibaba Cloud},
  year={2025},
  howpublished={\url{https://github.com/QwenLM/Qwen3}}
}
```
