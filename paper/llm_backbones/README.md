# LLM Backbones - ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸

> ğŸ’¡ **í•µì‹¬ ì§ˆë¬¸**: ì–´ë–¤ ì–¸ì–´ ëª¨ë¸ì´ í•œêµ­ì–´ë¥¼ ê°€ì¥ ìì—°ìŠ¤ëŸ½ê²Œ ìƒì„±í•˜ëŠ”ê°€?

VLMì—ì„œ í…ìŠ¤íŠ¸ ìƒì„±ì„ ë‹´ë‹¹í•˜ëŠ” LLM Backboneì˜ ë°œì „ íë¦„ì„ ì •ë¦¬í•©ë‹ˆë‹¤.

---

## ğŸ¯ ì´ ì¹´í…Œê³ ë¦¬ì˜ ëª©í‘œ

LLMì€ Visual Tokensì„ ë°›ì•„ **ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ìº¡ì…˜**ì„ ìƒì„±í•©ë‹ˆë‹¤. í•œêµ­ì–´ ì„±ëŠ¥ì´ í”„ë¡œì íŠ¸ì˜ í•µì‹¬ì…ë‹ˆë‹¤.

```mermaid
flowchart LR
    subgraph Input["ì…ë ¥"]
        VT["ğŸ”¢ Visual Tokens<br/>(Vision Encoderì—ì„œ)"]
        PT["ğŸ“ Prompt<br/>'ì´ ì˜ìƒì„ ì„¤ëª…í•´ì£¼ì„¸ìš”'"]
    end

    subgraph LLM["LLM Backbone"]
        Model["Qwen3 / Vicuna<br/>Auto-regressive ìƒì„±"]
    end

    subgraph Output["ì¶œë ¥"]
        Caption["ğŸ“ í•œêµ­ì–´ ìº¡ì…˜<br/>'í‘¸ë¥¸ ë°”ë‹¤ ìœ„ë¡œ<br/>í•˜ì–€ íŒŒë„ê°€...'"]
    end

    VT --> Model
    PT --> Model
    Model --> Caption

    style Model fill:#69db7c,stroke:#2f9e44
```

---

## ğŸ“Š í•œêµ­ì–´ ì„±ëŠ¥ ë¹„êµ

### MMLU-Ko ë²¤ì¹˜ë§ˆí¬ (í•œêµ­ì–´ ì¶”ë¡  ëŠ¥ë ¥)

```mermaid
xychart-beta
    title "í•œêµ­ì–´ ë²¤ì¹˜ë§ˆí¬ ì„±ëŠ¥ ë¹„êµ"
    x-axis ["Vicuna-7B", "LLaMA-7B", "Qwen-7B", "Qwen2-7B", "Qwen3-8B", "Qwen3-14B"]
    y-axis "MMLU-Ko (%)" 30 --> 80
    bar [38.2, 34.5, 52.1, 62.1, 68.5, 72.3]
```

### í•µì‹¬ ì¸ì‚¬ì´íŠ¸

```mermaid
flowchart TB
    subgraph Insight["ğŸ’¡ í•µì‹¬ ë°œê²¬"]
        I1["Vicuna-7B (LLaVA ê¸°ë³¸)<br/>í•œêµ­ì–´ ì„±ëŠ¥: 38.2%<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"]
        I2["Qwen3-8B (ì—…ê·¸ë ˆì´ë“œ ëŒ€ìƒ)<br/>í•œêµ­ì–´ ì„±ëŠ¥: 68.5%<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"]
        I3["ì„±ëŠ¥ í–¥ìƒ: +79%<br/>ë¹„ìŠ·í•œ í¬ê¸°ë¡œ ê±°ì˜ 2ë°°!"]
    end

    I1 --> I3
    I2 --> I3

    style I1 fill:#ffe3e3
    style I2 fill:#d3f9d8
    style I3 fill:#fff3bf
```

---

## ğŸ“ˆ LLM ë°œì „ íë¦„

```mermaid
flowchart TB
    subgraph Era2023["2023ë…„: ì˜¤í”ˆì†ŒìŠ¤ LLM ì‹œëŒ€"]
        LLaMA["ğŸ¦™ LLaMA (Meta)<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>â€¢ ì˜¤í”ˆì†ŒìŠ¤ ì‹œì‘<br/>â€¢ 7B~65B íŒŒë¼ë¯¸í„°<br/>â€¢ ì˜ì–´ ì¤‘ì‹¬"]
        
        LLaMA --> Vicuna["ğŸ’¬ Vicuna<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>â€¢ ShareGPT í•™ìŠµ<br/>â€¢ ëŒ€í™” íŠ¹í™”<br/>â€¢ LLaVA ê¸°ë³¸ LLM"]
        
        LLaMA --> Qwen["ğŸŒ Qwen (Alibaba)<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>â€¢ ë‹¤êµ­ì–´ íŠ¹í™”<br/>â€¢ í•œêµ­ì–´ ì„±ëŠ¥ â†‘<br/>â€¢ 8K ì»¨í…ìŠ¤íŠ¸"]
    end

    subgraph Era2024["2024ë…„: ì„±ëŠ¥ í–¥ìƒ"]
        LLaMA --> LLaMA3["ğŸ¦™ LLaMA 3<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>â€¢ 8B~400B<br/>â€¢ ì„±ëŠ¥ ëŒ€í­ í–¥ìƒ"]
        
        Qwen --> Qwen2["ğŸŒ Qwen2<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>â€¢ GQA ì ìš©<br/>â€¢ 128K ì»¨í…ìŠ¤íŠ¸<br/>â€¢ ë©”ëª¨ë¦¬ íš¨ìœ¨ â†‘"]
    end

    subgraph Era2025["2025ë…„: ìµœì‹ "]
        Qwen2 --> Qwen3["â­ Qwen3<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>â€¢ MoE ì§€ì›<br/>â€¢ 0.6B~235B<br/>â€¢ ìµœê³  ì„±ëŠ¥"]
    end

    subgraph Project["ğŸ¯ ìš°ë¦¬ ì„ íƒ"]
        Choice["Vicuna â†’ Qwen3<br/>í•œêµ­ì–´ ì„±ëŠ¥ 2ë°° â†‘"]
    end

    Vicuna -.-> Choice
    Qwen3 ==> Choice

    style Qwen3 fill:#69db7c,stroke:#2f9e44,stroke-width:3px
    style Choice fill:#ff6b6b,stroke:#c92a2a,color:#fff
```

---

## ğŸ”¬ ì•„í‚¤í…ì²˜ ìƒì„¸

### Attention ë©”ì»¤ë‹ˆì¦˜ ì§„í™”

```mermaid
flowchart TB
    subgraph MHA["MHA (LLaMA, Vicuna)"]
        MHA_Desc["Multi-Head Attention<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Q, K, V ëª¨ë‘ ë™ì¼í•œ head ìˆ˜<br/>ì˜ˆ: 32 heads ì „ë¶€"]
        MHA_KV["KV Cache í¬ê¸°<br/>32 Ã— head_dim Ã— seq_len<br/>= í¼"]
    end

    subgraph GQA["GQA (Qwen2, Qwen3)"]
        GQA_Desc["Grouped Query Attention<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Q: 32 heads<br/>K, V: 8 heads (ê·¸ë£¹ ê³µìœ )"]
        GQA_KV["KV Cache í¬ê¸°<br/>8 Ã— head_dim Ã— seq_len<br/>= 4ë°° ê°ì†Œ!"]
    end

    MHA --> |ë°œì „| GQA

    subgraph Benefit["ì¥ì "]
        B["â€¢ ê¸´ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ ê°€ëŠ¥<br/>â€¢ ë©”ëª¨ë¦¬ íš¨ìœ¨ í–¥ìƒ<br/>â€¢ ì†ë„ í–¥ìƒ"]
    end

    GQA --> Benefit

    style GQA fill:#d3f9d8
    style Benefit fill:#fff3bf
```

### MoE (Mixture of Experts) - Qwen3

```mermaid
flowchart TB
    subgraph Dense["Dense Model (ì¼ë°˜)"]
        D_Input["ì…ë ¥"] --> D_All["ëª¨ë“  íŒŒë¼ë¯¸í„°<br/>í™œì„±í™”"]
        D_All --> D_Output["ì¶œë ¥"]
        D_Note["8B ëª¨ë¸ = 8B ì—°ì‚°"]
    end

    subgraph MoE["MoE Model (Qwen3-30B-A3B)"]
        M_Input["ì…ë ¥"] --> Router["Router<br/>(ì–´ë–¤ Expert?)"]
        
        Router --> E1["Expert 1"]
        Router --> E2["Expert 2<br/>âœ“ ì„ íƒ"]
        Router --> E3["Expert 3"]
        Router --> E4["Expert 4<br/>âœ“ ì„ íƒ"]
        Router --> E5["..."]
        Router --> E8["Expert 8"]
        
        E2 --> M_Output["ì¶œë ¥"]
        E4 --> M_Output
        
        M_Note["30B ì´ íŒŒë¼ë¯¸í„°<br/>3Bë§Œ í™œì„±í™”<br/>= Dense 3B ì—°ì‚°ëŸ‰ìœ¼ë¡œ<br/>30Bê¸‰ ì„±ëŠ¥!"]
    end

    style E2 fill:#d3f9d8
    style E4 fill:#d3f9d8
    style M_Note fill:#fff3bf
```

---

## ğŸ“Š ëª¨ë¸ë³„ ìƒì„¸ ë¹„êµ

### Qwen3 ë¼ì¸ì—…

```mermaid
flowchart TB
    subgraph Dense["Dense Models"]
        Q06["Qwen3-0.6B<br/>CPU ê°€ëŠ¥"]
        Q17["Qwen3-1.7B<br/>Draftìš©"]
        Q4["Qwen3-4B<br/>T4 ê¶Œì¥"]
        Q8["Qwen3-8B<br/>L4 ê¶Œì¥"]
        Q14["Qwen3-14B<br/>A100 ê¶Œì¥"]
        Q32["Qwen3-32B<br/>H100 ê¶Œì¥"]
    end

    subgraph MoE_Models["MoE Models"]
        Q30A3["Qwen3-30B-A3B<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>30B ì´ íŒŒë¼ë¯¸í„°<br/>3B í™œì„±í™”<br/>íš¨ìœ¨ì !"]
        Q235A22["Qwen3-235B-A22B<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>235B ì´ íŒŒë¼ë¯¸í„°<br/>22B í™œì„±í™”<br/>ìµœê³  ì„±ëŠ¥!"]
    end

    style Q8 fill:#d3f9d8
    style Q14 fill:#4dabf7
    style Q30A3 fill:#fff3bf
```

### ìƒì„¸ ë¹„êµí‘œ

| ëª¨ë¸ | íŒŒë¼ë¯¸í„° | ì»¨í…ìŠ¤íŠ¸ | í•œêµ­ì–´ | ë¼ì´ì„ ìŠ¤ | GPU ê¶Œì¥ |
|------|----------|----------|--------|----------|----------|
| Vicuna-7B | 7B | 4K | 38.2% | ì—°êµ¬ìš© | T4 |
| Qwen-7B | 7B | 32K | 52.1% | ì¼ë¶€ ìƒì—… | T4 |
| Qwen2-7B | 7B | 128K | 62.1% | Apache-2.0 | L4 |
| **Qwen3-4B** | 4B | 32K | ~60% | Apache-2.0 | **T4** |
| **Qwen3-8B** | 8B | 128K | 68.5% | Apache-2.0 | **L4** |
| **Qwen3-14B** | 14B | 128K | 72.3% | Apache-2.0 | **A100** |
| **Qwen3-32B** | 32B | 128K | ~75% | Apache-2.0 | **H100** |

---

## ğŸ¯ ìš°ë¦¬ í”„ë¡œì íŠ¸ ì ìš©

### LLM êµì²´ ê²°ì • íŠ¸ë¦¬

```mermaid
flowchart TB
    Start["LLM ì„ íƒ"] --> Q1{"GPU ì¢…ë¥˜?"}
    
    Q1 -->|"T4 (16GB)"| T4_Choice["Qwen3-4B-Instruct<br/>4-bit: ~3GB"]
    
    Q1 -->|"L4 (24GB)"| L4_Choice["Qwen3-8B-Instruct<br/>4-bit: ~5GB"]
    
    Q1 -->|"A100 (40GB)"| A100_Choice["Qwen3-14B-Instruct<br/>4-bit: ~8GB"]
    
    Q1 -->|"H100 (80GB)"| H100_Q{"MoE ì‚¬ìš©?"}
    H100_Q -->|"ì˜ˆ"| H100_MoE["Qwen3-30B-A3B<br/>íš¨ìœ¨ì  + ê³ ì„±ëŠ¥"]
    H100_Q -->|"ì•„ë‹ˆì˜¤"| H100_Dense["Qwen3-32B-Instruct<br/>ìµœê³  ì„±ëŠ¥"]

    style T4_Choice fill:#fff3bf
    style L4_Choice fill:#d3f9d8
    style A100_Choice fill:#4dabf7
    style H100_MoE fill:#e5dbff
```

### LLM êµì²´ ì‹œ í•„ìš”í•œ ì‘ì—…

```mermaid
flowchart TB
    subgraph Change["Vicuna â†’ Qwen3 êµì²´"]
        C1["ê¸°ì¡´: LLaVA + Vicuna-7B"]
        C2["ë³€ê²½: LLaVA + Qwen3-8B"]
    end

    subgraph Tasks["í•„ìš” ì‘ì—…"]
        T1["1ï¸âƒ£ Projector ì¬í•™ìŠµ<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Vision ì¶œë ¥ â†’ Qwen ì…ë ¥<br/>ì°¨ì› ì •ë ¬ í•„ìš”"]
        
        T2["2ï¸âƒ£ í”„ë¡¬í”„íŠ¸ í˜•ì‹ ë³€ê²½<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Vicuna: USER: ... ASSISTANT:<br/>Qwen3: <|im_start|>..."]
        
        T3["3ï¸âƒ£ í† í¬ë‚˜ì´ì € ë³€ê²½<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>ì–´íœ˜ í¬ê¸°, special tokens"]
        
        T4["4ï¸âƒ£ í•™ìŠµ ì„¤ì • ì¡°ì •<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>LR, batch size ë“±"]
    end

    Change --> T1 --> T2 --> T3 --> T4

    style T1 fill:#ffe3e3
    style T2 fill:#fff3bf
```

### í”„ë¡¬í”„íŠ¸ í˜•ì‹ ë¹„êµ

#### Vicuna (LLaVA ê¸°ë³¸)
```
USER: <video>ì´ ì˜ìƒì„ í•œêµ­ì–´ë¡œ ìƒì„¸íˆ ë¬˜ì‚¬í•´ì£¼ì„¸ìš”.
ASSISTANT: ì´ ì˜ìƒì€ í‘¸ë¥¸ ë°”ë‹¤ì™€...
```

#### Qwen3 (ì—…ê·¸ë ˆì´ë“œ)
```
<|im_start|>system
ë‹¹ì‹ ì€ ë¹„ë””ì˜¤ ìº¡ì…”ë‹ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.<|im_end|>
<|im_start|>user
<video>ì´ ì˜ìƒì„ í•œêµ­ì–´ë¡œ ìƒì„¸íˆ ë¬˜ì‚¬í•´ì£¼ì„¸ìš”.<|im_end|>
<|im_start|>assistant
ì´ ì˜ìƒì€ í‘¸ë¥¸ ë°”ë‹¤ì™€...<|im_end|>
```

### ì½”ë“œ ì˜ˆì‹œ

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Qwen3 ë¡œë“œ
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen3-8B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-8B-Instruct")

# ëŒ€í™” í˜•ì‹ ì ìš©
messages = [
    {"role": "system", "content": "ë‹¹ì‹ ì€ ë¹„ë””ì˜¤ ìº¡ì…”ë‹ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
    {"role": "user", "content": "ì´ ì˜ìƒì„ í•œêµ­ì–´ë¡œ ìƒì„¸íˆ ë¬˜ì‚¬í•´ì£¼ì„¸ìš”."}
]
text = tokenizer.apply_chat_template(messages, tokenize=False)
```

---

## ğŸ“š ë…¼ë¬¸ ëª©ë¡

| íŒŒì¼ | ë…¼ë¬¸ | í•µì‹¬ í¬ì¸íŠ¸ | ì¤‘ìš”ë„ |
|------|------|------------|--------|
| [llama.md](llama.md) | LLaMA (2023) | ì˜¤í”ˆì†ŒìŠ¤ LLM ê¸°ì´ˆ | â­â­â­ |
| [qwen.md](qwen.md) | Qwen (2023) | ë‹¤êµ­ì–´ íŠ¹í™” ì‹œì‘ | â­â­â­ |
| [qwen2.md](qwen2.md) | Qwen2 (2024) | GQA, 128K ì»¨í…ìŠ¤íŠ¸ | â­â­â­â­ |
| [qwen3.md](qwen3.md) | Qwen3 (2025) | **ê¶Œì¥ ì—…ê·¸ë ˆì´ë“œ ëŒ€ìƒ** | â­â­â­â­â­ |

---

## ğŸ’» GPUë³„ ê¶Œì¥

| GPU | ê¶Œì¥ LLM | 4-bit ë©”ëª¨ë¦¬ | í•œêµ­ì–´ ì„±ëŠ¥ |
|-----|----------|-------------|------------|
| **T4 (16GB)** | Qwen3-4B-Instruct | ~3GB | ~60% |
| **L4 (24GB)** | Qwen3-8B-Instruct | ~5GB | 68.5% |
| **A100 (40GB)** | Qwen3-14B-Instruct | ~8GB | 72.3% |
| **H100 (80GB)** | Qwen3-32B-Instruct | ~18GB | ~75% |
