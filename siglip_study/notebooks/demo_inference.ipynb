{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Korean Video Captioning Demo\n",
    "\n",
    "**\ubaa9\uc801**: \ud559\uc2b5\ub41c \ubaa8\ub378\ub85c \uc0d8\ud50c \ube44\ub514\uc624\uc5d0 \ub300\ud55c \ud55c\uad6d\uc5b4 \ucea1\uc158 \uc0dd\uc131 \uc2dc\uc5f0\n",
    "\n",
    "**\ubaa8\ub378 \uad6c\uc870**: CLIP-ViT-L/14 (Vision Encoder) + Projector + Qwen3-8B (LLM + LoRA)\n",
    "\n",
    "**\uccb4\ud06c\ud3ec\uc778\ud2b8**: `siglip_study/results3/` \uc5d0\uc11c \ud559\uc2b5\ub41c \ubaa8\ub378\n",
    "\n",
    "**\ub370\uc774\ud130**: AI-Hub \ub300\ud55c\ubbfc\uad6d \ubc30\uacbd\uc601\uc0c1 (aihub_splitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Google Drive \ub9c8\uc6b4\ud2b8 \ubc0f \ud658\uacbd \uc124\uc815"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive \ub9c8\uc6b4\ud2b8\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# \uacbd\ub85c \uc124\uc815\n",
    "DRIVE_ROOT = \"/content/drive/MyDrive/mutsa-02\"\n",
    "DATA_PATH = f\"{DRIVE_ROOT}/aihub_splitted\"\n",
    "RESULTS_DIR = f\"{DRIVE_ROOT}/korean_video_captioning/siglip_study/results3\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"DRIVE_ROOT:   {DRIVE_ROOT}\")\n",
    "print(f\"DATA_PATH:    {DATA_PATH}\")\n",
    "print(f\"RESULTS_DIR:  {RESULTS_DIR}\")\n",
    "\n",
    "# \ub370\uc774\ud130 \uad6c\uc870 \ud655\uc778\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Data Structure Check:\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if os.path.exists(DATA_PATH):\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        split_path = Path(DATA_PATH) / split\n",
    "        if split_path.exists():\n",
    "            items = list(split_path.iterdir())\n",
    "            print(f\"\\n{split}/ ({len(items)} items):\")\n",
    "            \n",
    "            # labels/videos \ud3f4\ub354\uac00 \uc788\ub294\uc9c0 \ud655\uc778\n",
    "            has_labels = (split_path / \"labels\").exists()\n",
    "            has_videos = (split_path / \"videos\").exists()\n",
    "            \n",
    "            if has_labels and has_videos:\n",
    "                labels = list((split_path / \"labels\").glob(\"*.json\"))\n",
    "                videos = list((split_path / \"videos\").glob(\"*.mp4\"))\n",
    "                print(f\"  Structure: labels/videos folders\")\n",
    "                print(f\"  Labels: {len(labels)}, Videos: {len(videos)}\")\n",
    "            else:\n",
    "                # \uc0d8\ud50c \ud3f4\ub354\ub4e4\uc778\uc9c0 \ud655\uc778\n",
    "                sample_dirs = [d for d in items if d.is_dir()]\n",
    "                print(f\"  Structure: {len(sample_dirs)} sample folders\")\n",
    "                if sample_dirs:\n",
    "                    sample = sample_dirs[0]\n",
    "                    print(f\"  Example: {sample.name}/\")\n",
    "                    for f in sample.iterdir():\n",
    "                        print(f\"    - {f.name}\")\n",
    "    print(f\"\\n\u2705 Data found!\")\n",
    "else:\n",
    "    print(f\"\\n\u274c Data path not found!\")\n",
    "\n",
    "# \uc0ac\uc6a9 \uac00\ub2a5\ud55c \uc2e4\ud5d8 \ud655\uc778\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Available experiments:\")\n",
    "print(f\"{'='*60}\")\n",
    "EXPERIMENTS = [\n",
    "    (\"E1_v2_linear_optimized\", \"linear\"),\n",
    "    (\"E3_v2_mlp_optimized\", \"mlp_2l\"),\n",
    "    (\"E5_v2_lr_reduced\", \"c_abstractor\"),\n",
    "    (\"E5_v3_epoch_increased\", \"c_abstractor\"),\n",
    "    (\"E7_v2_perceiver_optimized\", \"perceiver\"),\n",
    "]\n",
    "for exp_name, proj_type in EXPERIMENTS:\n",
    "    ckpt_path = f\"{RESULTS_DIR}/{exp_name}/checkpoints/best_model.pt\"\n",
    "    status = \"\u2705\" if os.path.exists(ckpt_path) else \"\u274c\"\n",
    "    print(f\"  {status} {exp_name} ({proj_type})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud544\uc694\ud55c \ud328\ud0a4\uc9c0 \uc124\uce58 (Colab)\n",
    "!pip install -q transformers accelerate bitsandbytes peft\n",
    "!pip install -q opencv-python-headless\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. \ubaa8\ub378 \uc815\uc758 (siglip_study\uc640 \ub3d9\uc77c\ud55c \uad6c\uc870)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    CLIPVisionModel, CLIPImageProcessor,\n",
    "    AutoModelForCausalLM, AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# \uc124\uc815 (siglip_study\uc640 \ub3d9\uc77c)\n",
    "CONFIG = {\n",
    "    \"vision_encoder\": \"openai/clip-vit-large-patch14-336\",\n",
    "    \"llm\": \"Qwen/Qwen3-8B\",\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"num_frames\": 8,\n",
    "    \"max_length\": 768,\n",
    "    \"num_queries\": 64,  # C-Abstractor, Perceiver\uc6a9\n",
    "    \"num_heads\": 8,\n",
    "    \"num_layers\": 2,\n",
    "    \"prompt\": \"\uc774 \uc601\uc0c1\uc744 \uc790\uc138\ud788 \uc124\uba85\ud574\uc8fc\uc138\uc694.\",\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"data_path\": DATA_PATH,\n",
    "    \"results_dir\": RESULTS_DIR,\n",
    "}\n",
    "\n",
    "print(\"Config loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Projector \uc815\uc758 (siglip_study\uc640 \ub3d9\uc77c\ud55c \uad6c\uc870)\n",
    "# ============================================\n",
    "\n",
    "class LinearProjector(nn.Module):\n",
    "    \"\"\"Linear Projector (4M params)\"\"\"\n",
    "    def __init__(self, vision_dim=1024, llm_dim=4096):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(vision_dim, llm_dim)\n",
    "    def forward(self, x):\n",
    "        return self.proj(x)\n",
    "\n",
    "class MLPProjector(nn.Module):\n",
    "    \"\"\"MLP-2L Projector (8M params)\"\"\"\n",
    "    def __init__(self, vision_dim=1024, llm_dim=4096, hidden_dim=4096):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(vision_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, llm_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.proj(x)\n",
    "\n",
    "class CAbstractor(nn.Module):\n",
    "    \"\"\"C-Abstractor Projector (206M params)\"\"\"\n",
    "    def __init__(self, num_queries=64, vision_dim=1024, llm_dim=4096, num_heads=8, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.queries = nn.Parameter(torch.randn(num_queries, llm_dim) * 0.02)\n",
    "        self.vision_proj = nn.Linear(vision_dim, llm_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerDecoderLayer(d_model=llm_dim, nhead=num_heads, dim_feedforward=llm_dim*4, batch_first=True)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(llm_dim)\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        x = self.vision_proj(x)\n",
    "        q = self.queries.unsqueeze(0).expand(B, -1, -1)\n",
    "        for layer in self.layers:\n",
    "            q = layer(q, x)\n",
    "        return self.norm(q)\n",
    "\n",
    "# ============================================\n",
    "# Perceiver Resampler (siglip_study \uc6d0\ubcf8 \uad6c\uc870)\n",
    "# ============================================\n",
    "class PerceiverLayer(nn.Module):\n",
    "    \"\"\"Perceiver Layer with Self-Attention + Cross-Attention\"\"\"\n",
    "    def __init__(self, dim, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.cross_attn = nn.MultiheadAttention(dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.norm3 = nn.LayerNorm(dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(dim, dim*4), \n",
    "            nn.GELU(), \n",
    "            nn.Dropout(dropout), \n",
    "            nn.Linear(dim*4, dim), \n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, queries, context):\n",
    "        q = self.norm1(queries + self.self_attn(queries, queries, queries)[0])\n",
    "        q = self.norm2(q + self.cross_attn(q, context, context)[0])\n",
    "        return self.norm3(q + self.ffn(q))\n",
    "\n",
    "class PerceiverResampler(nn.Module):\n",
    "    \"\"\"Perceiver Resampler Projector (siglip_study \uc6d0\ubcf8 \uad6c\uc870)\"\"\"\n",
    "    def __init__(self, vision_dim=1024, llm_dim=4096, num_queries=64, num_heads=8, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.queries = nn.Parameter(torch.randn(num_queries, llm_dim) * 0.02)\n",
    "        self.input_proj = nn.Linear(vision_dim, llm_dim)\n",
    "        self.layers = nn.ModuleList([PerceiverLayer(llm_dim, num_heads, dropout) for _ in range(num_layers)])\n",
    "        self.output_norm = nn.LayerNorm(llm_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [B, seq_len, vision_dim]\n",
    "        B = x.size(0)\n",
    "        context = self.input_proj(x)  # [B, seq_len, llm_dim]\n",
    "        queries = self.queries.unsqueeze(0).expand(B, -1, -1)  # [B, num_queries, llm_dim]\n",
    "        for layer in self.layers:\n",
    "            queries = layer(queries, context)\n",
    "        return self.output_norm(queries)  # [B, num_queries, llm_dim]\n",
    "\n",
    "\n",
    "def create_projector(projector_type, vision_dim=1024, llm_dim=4096, config=None):\n",
    "    \"\"\"Projector \uc0dd\uc131\"\"\"\n",
    "    if projector_type == \"linear\":\n",
    "        return LinearProjector(vision_dim, llm_dim)\n",
    "    elif projector_type == \"mlp_2l\":\n",
    "        return MLPProjector(vision_dim, llm_dim)\n",
    "    elif projector_type == \"c_abstractor\":\n",
    "        return CAbstractor(\n",
    "            num_queries=config.get(\"num_queries\", 64),\n",
    "            vision_dim=vision_dim,\n",
    "            llm_dim=llm_dim,\n",
    "            num_heads=config.get(\"num_heads\", 8),\n",
    "            num_layers=config.get(\"num_layers\", 2)\n",
    "        )\n",
    "    elif projector_type == \"perceiver\":\n",
    "        return PerceiverResampler(\n",
    "            vision_dim=vision_dim,\n",
    "            llm_dim=llm_dim,\n",
    "            num_queries=config.get(\"num_queries\", 64),\n",
    "            num_heads=config.get(\"num_heads\", 8),\n",
    "            num_layers=config.get(\"num_layers\", 2)\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown projector type: {projector_type}\")\n",
    "\n",
    "print(\"Projector classes defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CustomVLM \uc815\uc758 (dtype \ucc98\ub9ac \ud3ec\ud568)\n",
    "# ============================================\n",
    "\n",
    "class CustomVLM(nn.Module):\n",
    "    \"\"\"Vision-Language Model: CLIP + Projector + Qwen3-8B\"\"\"\n",
    "    def __init__(self, vision_encoder, projector, llm, tokenizer):\n",
    "        super().__init__()\n",
    "        self.vision_encoder = vision_encoder\n",
    "        self.projector = projector\n",
    "        self.llm = llm\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def get_vision_features(self, pixel_values):\n",
    "        \"\"\"Vision Encoder\ub85c \ud2b9\uc9d5 \ucd94\ucd9c\"\"\"\n",
    "        with torch.no_grad():\n",
    "            outputs = self.vision_encoder(pixel_values=pixel_values)\n",
    "            # [CLS] \ud1a0\ud070 \uc81c\uc678\n",
    "            features = outputs.last_hidden_state[:, 1:, :]  # [B, 576, 1024]\n",
    "        return features\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, pixel_values, prompt, max_new_tokens=256, **kwargs):\n",
    "        \"\"\"\ucea1\uc158 \uc0dd\uc131\"\"\"\n",
    "        device = pixel_values.device\n",
    "        \n",
    "        # 1. Vision features \ucd94\ucd9c\n",
    "        vision_features = self.get_vision_features(pixel_values)\n",
    "        \n",
    "        # 2. \ud504\ub808\uc784\ubcc4 features\ub97c \ud558\ub098\ub85c \ud569\uce68\n",
    "        # pixel_values: [num_frames, C, H, W] \u2192 vision_features: [num_frames, 576, 1024]\n",
    "        # \u2192 reshape to [1, num_frames * 576, 1024]\n",
    "        vision_features = vision_features.reshape(1, -1, vision_features.size(-1))  # [1, num_frames*576, 1024]\n",
    "        \n",
    "        # 3. Projector\ub85c \ubcc0\ud658 (dtype \ub9de\ucd94\uae30)\n",
    "        projected = self.projector(vision_features.float())  # [1, N, 4096]\n",
    "        \n",
    "        # 4. \ud14d\uc2a4\ud2b8 \ud1a0\ud070\ud654\n",
    "        text_inputs = self.tokenizer(\n",
    "            [prompt],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        ).to(device)\n",
    "        \n",
    "        # 5. \ud14d\uc2a4\ud2b8 \uc784\ubca0\ub529\n",
    "        text_embeds = self.llm.get_input_embeddings()(text_inputs.input_ids)\n",
    "        \n",
    "        # 6. Vision + Text \uacb0\ud569 (dtype \ub9de\ucd94\uae30)\n",
    "        projected = projected.to(text_embeds.dtype)\n",
    "        inputs_embeds = torch.cat([projected, text_embeds], dim=1)\n",
    "        \n",
    "        # 7. Attention mask \uc0dd\uc131\n",
    "        vision_mask = torch.ones(1, projected.size(1), device=device, dtype=text_inputs.attention_mask.dtype)\n",
    "        attention_mask = torch.cat([vision_mask, text_inputs.attention_mask], dim=1)\n",
    "        \n",
    "        # 8. \uc0dd\uc131\n",
    "        gen_kwargs = {\n",
    "            \"inputs_embeds\": inputs_embeds,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "            \"do_sample\": False,\n",
    "            \"pad_token_id\": self.tokenizer.pad_token_id,\n",
    "            \"eos_token_id\": self.tokenizer.eos_token_id,\n",
    "        }\n",
    "        \n",
    "        outputs = self.llm.generate(**gen_kwargs)\n",
    "        \n",
    "        # 9. \ub514\ucf54\ub529\n",
    "        captions = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        return captions\n",
    "\n",
    "print(\"CustomVLM class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. \ubaa8\ub378 \ub85c\ub4dc \ud568\uc218"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(config, projector_type, device, resume_path=None):\n",
    "    \"\"\"\n",
    "    \ubaa8\ub378 \ube4c\ub4dc \ubc0f \uccb4\ud06c\ud3ec\uc778\ud2b8 \ub85c\ub4dc\n",
    "    siglip_study \ub178\ud2b8\ubd81\uacfc \ub3d9\uc77c\ud55c \ubc29\uc2dd\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Building model with {projector_type} projector...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # 1. Vision Encoder (CLIP)\n",
    "    print(\"Loading Vision Encoder...\")\n",
    "    vision_encoder = CLIPVisionModel.from_pretrained(config[\"vision_encoder\"]).to(device)\n",
    "    image_processor = CLIPImageProcessor.from_pretrained(config[\"vision_encoder\"])\n",
    "    vision_encoder.eval()\n",
    "    for param in vision_encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "    print(f\"  \u2705 Vision Encoder loaded (frozen)\")\n",
    "    \n",
    "    # 2. LLM (Qwen3-8B with 4-bit quantization)\n",
    "    print(\"Loading LLM...\")\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    llm = AutoModelForCausalLM.from_pretrained(\n",
    "        config[\"llm\"],\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config[\"llm\"], trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(f\"  \u2705 LLM loaded (4-bit quantized)\")\n",
    "    print(f\"  PAD: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "    print(f\"  EOS: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
    "    \n",
    "    # 3. Projector \uc0dd\uc131\n",
    "    print(f\"Creating {projector_type} Projector...\")\n",
    "    projector = create_projector(\n",
    "        projector_type,\n",
    "        vision_dim=1024,\n",
    "        llm_dim=llm.config.hidden_size,\n",
    "        config=config\n",
    "    ).to(device)\n",
    "    proj_params = sum(p.numel() for p in projector.parameters())\n",
    "    print(f\"  \u2705 Projector created ({proj_params:,} params)\")\n",
    "    \n",
    "    # 4. LoRA \uc801\uc6a9\n",
    "    print(\"Applying LoRA...\")\n",
    "    lora_config = LoraConfig(\n",
    "        r=config[\"lora_r\"],\n",
    "        lora_alpha=config[\"lora_alpha\"],\n",
    "        lora_dropout=config[\"lora_dropout\"],\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "    )\n",
    "    llm = get_peft_model(llm, lora_config)\n",
    "    llm.print_trainable_parameters()\n",
    "    \n",
    "    # 5. CustomVLM \uc870\ub9bd\n",
    "    model = CustomVLM(vision_encoder, projector, llm, tokenizer)\n",
    "    \n",
    "    # 6. \uccb4\ud06c\ud3ec\uc778\ud2b8 \ub85c\ub4dc\n",
    "    if resume_path and Path(resume_path).exists():\n",
    "        print(f\"\\nLoading checkpoint: {resume_path}\")\n",
    "        checkpoint = torch.load(resume_path, map_location=device, weights_only=False)\n",
    "        \n",
    "        # Projector \uac00\uc911\uce58 \ub85c\ub4dc\n",
    "        if \"projector_state_dict\" in checkpoint:\n",
    "            model.projector.load_state_dict(checkpoint[\"projector_state_dict\"])\n",
    "            print(\"  \u2705 Projector weights loaded\")\n",
    "        \n",
    "        # LoRA \uac00\uc911\uce58 \ub85c\ub4dc\n",
    "        if \"lora_state_dict\" in checkpoint:\n",
    "            # PEFT \ubaa8\ub378\uc5d0 LoRA \uac00\uc911\uce58 \uc801\uc6a9\n",
    "            missing, unexpected = model.llm.load_state_dict(checkpoint[\"lora_state_dict\"], strict=False)\n",
    "            print(f\"  \u2705 LoRA weights loaded (missing: {len(missing)}, unexpected: {len(unexpected)})\")\n",
    "        \n",
    "        print(\"  \u2705 Checkpoint loaded successfully!\")\n",
    "    else:\n",
    "        print(f\"\\n\u26a0\ufe0f No checkpoint found at {resume_path}\")\n",
    "        print(\"  Using randomly initialized weights...\")\n",
    "    \n",
    "    model.eval()\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"\u2705 Model ready for inference!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return model, vision_encoder, image_processor, tokenizer\n",
    "\n",
    "\n",
    "def load_trained_model(exp_name, projector_type, config):\n",
    "    \"\"\"\n",
    "    \ud559\uc2b5\ub41c \ubaa8\ub378 \ub85c\ub4dc (\ud3b8\uc758 \ud568\uc218)\n",
    "    \"\"\"\n",
    "    ckpt_path = Path(config[\"results_dir\"]) / exp_name / \"checkpoints\" / \"best_model.pt\"\n",
    "    if not ckpt_path.exists():\n",
    "        print(f\"\u274c Checkpoint not found: {ckpt_path}\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    model, vision_encoder, img_proc, tok = build_model(config, projector_type, \"cuda\", ckpt_path)\n",
    "    model.eval()\n",
    "    return model, vision_encoder, img_proc, tok\n",
    "\n",
    "print(\"Model loading functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. \ub370\uc774\ud130 \ub85c\ub4dc \uc720\ud2f8\ub9ac\ud2f0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_frames(video_path, num_frames=8):\n",
    "    \"\"\"\ube44\ub514\uc624\uc5d0\uc11c \uade0\ub4f1 \uac04\uaca9\uc73c\ub85c \ud504\ub808\uc784 \ucd94\ucd9c\"\"\"\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    if total_frames == 0:\n",
    "        cap.release()\n",
    "        return []\n",
    "    \n",
    "    indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "    \n",
    "    frames = []\n",
    "    for idx in indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(Image.fromarray(frame_rgb))\n",
    "    \n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "\n",
    "def load_sample(data_path, split=\"val\", idx=0):\n",
    "    \"\"\"\n",
    "    \uc0d8\ud50c \ub370\uc774\ud130 \ub85c\ub4dc - \ub450 \uac00\uc9c0 \uad6c\uc870 \uc9c0\uc6d0\n",
    "    \n",
    "    \uad6c\uc870 1 (siglip_study \uae30\ubcf8):\n",
    "      - {split}/labels/*.json\n",
    "      - {split}/videos/*.mp4\n",
    "      \n",
    "    \uad6c\uc870 2 (\ud3f4\ub354\ubcc4 \uc0d8\ud50c):\n",
    "      - {split}/{sample_id}/video.mp4 \ub610\ub294 {sample_id}.mp4\n",
    "      - {split}/{sample_id}/label.json \ub610\ub294 {sample_id}.json\n",
    "    \"\"\"\n",
    "    split_path = Path(data_path) / split\n",
    "    \n",
    "    if not split_path.exists():\n",
    "        print(f\"Split path not found: {split_path}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # \uad6c\uc870 1: labels/videos \ud3f4\ub354\uac00 \uc788\ub294 \uacbd\uc6b0\n",
    "    label_dir = split_path / \"labels\"\n",
    "    video_dir = split_path / \"videos\"\n",
    "    \n",
    "    if label_dir.exists() and video_dir.exists():\n",
    "        print(f\"Using structure 1: labels/videos folders\")\n",
    "        label_files = sorted(list(label_dir.glob(\"*.json\")))\n",
    "        \n",
    "        if idx >= len(label_files):\n",
    "            print(f\"Index {idx} out of range (total: {len(label_files)})\")\n",
    "            return None, None, None\n",
    "        \n",
    "        label_path = label_files[idx]\n",
    "        video_name = label_path.stem\n",
    "        video_path = video_dir / f\"{video_name}.mp4\"\n",
    "        \n",
    "    # \uad6c\uc870 2: \uac01 \uc0d8\ud50c\uc774 \ud3f4\ub354\uc778 \uacbd\uc6b0\n",
    "    else:\n",
    "        print(f\"Using structure 2: sample folders\")\n",
    "        sample_dirs = sorted([d for d in split_path.iterdir() if d.is_dir()])\n",
    "        \n",
    "        if idx >= len(sample_dirs):\n",
    "            print(f\"Index {idx} out of range (total: {len(sample_dirs)})\")\n",
    "            return None, None, None\n",
    "        \n",
    "        sample_dir = sample_dirs[idx]\n",
    "        video_name = sample_dir.name\n",
    "        \n",
    "        # \ube44\ub514\uc624 \ud30c\uc77c \ucc3e\uae30\n",
    "        video_path = None\n",
    "        for pattern in [\"video.mp4\", \"*.mp4\", \"video.avi\", \"*.avi\"]:\n",
    "            matches = list(sample_dir.glob(pattern))\n",
    "            if matches:\n",
    "                video_path = matches[0]\n",
    "                break\n",
    "        \n",
    "        if video_path is None:\n",
    "            print(f\"No video found in {sample_dir}\")\n",
    "            return None, None, None\n",
    "        \n",
    "        # \ub77c\ubca8 \ud30c\uc77c \ucc3e\uae30\n",
    "        label_path = None\n",
    "        for pattern in [\"label.json\", \"*.json\"]:\n",
    "            matches = list(sample_dir.glob(pattern))\n",
    "            if matches:\n",
    "                label_path = matches[0]\n",
    "                break\n",
    "    \n",
    "    # \ube44\ub514\uc624 \uc874\uc7ac \ud655\uc778\n",
    "    if video_path is None or not video_path.exists():\n",
    "        print(f\"Video not found: {video_path}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # \ub77c\ubca8 \ub85c\ub4dc\n",
    "    gt_caption = \"\"\n",
    "    if label_path and label_path.exists():\n",
    "        try:\n",
    "            with open(label_path, 'r', encoding='utf-8') as f:\n",
    "                label_data = json.load(f)\n",
    "            # \ub2e4\uc591\ud55c \uad6c\uc870 \uc2dc\ub3c4\n",
    "            if isinstance(label_data, dict):\n",
    "                # \uad6c\uc870 1: annotation.description_kr\n",
    "                gt_caption = label_data.get(\"annotation\", {}).get(\"description_kr\", \"\")\n",
    "                # \ub300\uccb4 \ud0a4\ub4e4\n",
    "                if not gt_caption:\n",
    "                    gt_caption = label_data.get(\"description_kr\", \"\")\n",
    "                if not gt_caption:\n",
    "                    gt_caption = label_data.get(\"description\", \"\")\n",
    "                if not gt_caption:\n",
    "                    gt_caption = label_data.get(\"caption\", \"\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading label: {e}\")\n",
    "    \n",
    "    print(f\"Loaded: {video_name}\")\n",
    "    print(f\"  Video: {video_path}\")\n",
    "    print(f\"  Caption length: {len(gt_caption)} chars\")\n",
    "    \n",
    "    return video_path, gt_caption, video_name\n",
    "\n",
    "\n",
    "def display_result(video_name, frames, generated_caption, gt_caption=None):\n",
    "    \"\"\"\uacb0\uacfc \uc2dc\uac01\ud654\"\"\"\n",
    "    # \ud504\ub808\uc784 \ud45c\uc2dc\n",
    "    n_frames = min(len(frames), 8)\n",
    "    if n_frames == 0:\n",
    "        print(\"No frames to display!\")\n",
    "        return\n",
    "        \n",
    "    fig, axes = plt.subplots(1, n_frames, figsize=(3*n_frames, 3))\n",
    "    \n",
    "    if n_frames == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (ax, frame) in enumerate(zip(axes, frames[:n_frames])):\n",
    "        ax.imshow(frame)\n",
    "        ax.set_title(f'Frame {i+1}', fontsize=10)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Video: {video_name}', fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # \ucea1\uc158 \ucd9c\ub825\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"\ud83d\udcdd Generated Caption:\")\n",
    "    print(\"-\"*70)\n",
    "    print(generated_caption)\n",
    "    \n",
    "    if gt_caption:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"\ud83d\udccb Ground Truth Caption:\")\n",
    "        print(\"-\"*70)\n",
    "        if len(gt_caption) > 500:\n",
    "            print(gt_caption[:500] + \"...\")\n",
    "        else:\n",
    "            print(gt_caption)\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "print(\"Data utilities defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. \ubaa8\ub378 \ub85c\ub4dc \ubc0f Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# \uc0ac\uc6a9\ud560 \uc2e4\ud5d8 \uc120\ud0dd (\uc0ac\uc6a9 \uac00\ub2a5\ud55c \ubaa8\ub378 \uc911 \uc120\ud0dd)\n",
    "# ============================================\n",
    "\n",
    "# \uc635\uc158 1: C-Abstractor (LR \uac10\uc18c \ubc84\uc804)\n",
    "EXP_NAME = \"E5_v2_lr_reduced\"\n",
    "PROJECTOR_TYPE = \"c_abstractor\"\n",
    "\n",
    "# \uc635\uc158 2: C-Abstractor (Epoch \uc99d\uac00 \ubc84\uc804)\n",
    "# EXP_NAME = \"E5_v3_epoch_increased\"\n",
    "# PROJECTOR_TYPE = \"c_abstractor\"\n",
    "\n",
    "# \uc635\uc158 3: Perceiver Resampler\n",
    "# EXP_NAME = \"E7_v2_perceiver_optimized\"\n",
    "# PROJECTOR_TYPE = \"perceiver\"\n",
    "\n",
    "# ============================================\n",
    "# \u26a0\ufe0f \ucc38\uace0: \n",
    "# - E1_v2 (Linear), E3_v2 (MLP)\ub294 \uc544\uc9c1 \ud559\uc2b5\ub418\uc9c0 \uc54a\uc74c\n",
    "# - C-Abstractor\ub294 Mode Collapse \uac00\ub2a5\uc131 \uc788\uc74c\n",
    "# ============================================\n",
    "\n",
    "print(f\"Selected experiment: {EXP_NAME}\")\n",
    "print(f\"Projector type: {PROJECTOR_TYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ubaa8\ub378 \ub85c\ub4dc\n",
    "model, vision_encoder, image_processor, tokenizer = load_trained_model(\n",
    "    EXP_NAME, PROJECTOR_TYPE, CONFIG\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. \uc0d8\ud50c \ube44\ub514\uc624 \ucea1\uc154\ub2dd (2\uac1c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption_for_video(model, image_processor, video_path, config):\n",
    "    \"\"\"\ube44\ub514\uc624\uc5d0 \ub300\ud55c \ucea1\uc158 \uc0dd\uc131\"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # \ud504\ub808\uc784 \ucd94\ucd9c\n",
    "    frames = extract_frames(video_path, num_frames=config[\"num_frames\"])\n",
    "    if not frames:\n",
    "        return \"Error: Could not extract frames\", []\n",
    "    \n",
    "    # \uc774\ubbf8\uc9c0 \uc804\ucc98\ub9ac\n",
    "    pixel_values = image_processor(\n",
    "        images=frames,\n",
    "        return_tensors=\"pt\"\n",
    "    ).pixel_values.to(device)  # [num_frames, 3, 336, 336]\n",
    "    \n",
    "    # \ubc30\uce58 \ucc28\uc6d0 \ucd94\uac00 (\ubaa8\ub378\uc774 [B, num_frames, C, H, W] \uae30\ub300\ud560 \uc218 \uc788\uc74c)\n",
    "    # \ud558\uc9c0\ub9cc \ud604\uc7ac \uad6c\uc870\ub294 [B * num_frames, C, H, W] \ud615\ud0dc\n",
    "    # generate \ud568\uc218\uc5d0\uc11c \ucc98\ub9ac\n",
    "    \n",
    "    # \ucea1\uc158 \uc0dd\uc131\n",
    "    with torch.no_grad():\n",
    "        captions = model.generate(\n",
    "            pixel_values,\n",
    "            prompt=config[\"prompt\"],\n",
    "            max_new_tokens=config[\"max_new_tokens\"],\n",
    "            repetition_penalty=config[\"repetition_penalty\"],\n",
    "        )\n",
    "    \n",
    "    return captions[0] if captions else \"Error: No caption generated\", frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \uc0d8\ud50c 1\n",
    "print(\"\\n\" + \"#\"*70)\n",
    "print(\"# SAMPLE 1\")\n",
    "print(\"#\"*70)\n",
    "\n",
    "video_path, gt_caption, video_name = load_sample(CONFIG[\"data_path\"], \"val\", idx=0)\n",
    "\n",
    "if video_path and video_path.exists():\n",
    "    print(f\"Video: {video_name}\")\n",
    "    generated, frames = generate_caption_for_video(model, image_processor, video_path, CONFIG)\n",
    "    display_result(video_name, frames, generated, gt_caption)\n",
    "else:\n",
    "    print(f\"Video not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \uc0d8\ud50c 2\n",
    "print(\"\\n\" + \"#\"*70)\n",
    "print(\"# SAMPLE 2\")\n",
    "print(\"#\"*70)\n",
    "\n",
    "video_path, gt_caption, video_name = load_sample(CONFIG[\"data_path\"], \"val\", idx=2)\n",
    "\n",
    "if video_path and video_path.exists():\n",
    "    print(f\"Video: {video_name}\")\n",
    "    generated, frames = generate_caption_for_video(model, image_processor, video_path, CONFIG)\n",
    "    display_result(video_name, frames, generated, gt_caption)\n",
    "else:\n",
    "    print(f\"Video not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. (\uc120\ud0dd) \ub2e4\ub978 \uc2e4\ud5d8 \ubaa8\ub378 \ube44\uad50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \uc5ec\ub7ec \ubaa8\ub378 \ube44\uad50 (\uc120\ud0dd \uc0ac\ud56d)\n",
    "def compare_models(video_idx=0):\n",
    "    \"\"\"\uc5ec\ub7ec Projector \ubaa8\ub378\uc758 \ucea1\uc158 \ube44\uad50\"\"\"\n",
    "    experiments = [\n",
    "        (\"E1_v2_linear_optimized\", \"linear\"),\n",
    "        (\"E3_v2_mlp_optimized\", \"mlp_2l\"),\n",
    "        # (\"E5_v2_lr_reduced\", \"c_abstractor\"),  # Mode Collapse \uc704\ud5d8\n",
    "    ]\n",
    "    \n",
    "    # \ube44\ub514\uc624 \ub85c\ub4dc\n",
    "    video_path, gt_caption, video_name = load_sample(CONFIG[\"data_path\"], \"val\", idx=video_idx)\n",
    "    frames = extract_frames(video_path, num_frames=CONFIG[\"num_frames\"])\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Comparing models on: {video_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # \ud504\ub808\uc784 \ud45c\uc2dc\n",
    "    fig, axes = plt.subplots(1, min(4, len(frames)), figsize=(12, 3))\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(frames[i])\n",
    "        ax.axis('off')\n",
    "    plt.suptitle(f'{video_name}', fontweight='bold')\n",
    "    plt.show()\n",
    "    \n",
    "    results = {}\n",
    "    for exp_name, proj_type in experiments:\n",
    "        print(f\"\\n--- {exp_name} ({proj_type}) ---\")\n",
    "        \n",
    "        # \ubaa8\ub378 \ub85c\ub4dc\n",
    "        m, ve, ip, tok = load_trained_model(exp_name, proj_type, CONFIG)\n",
    "        if m is None:\n",
    "            print(f\"  Skipped (checkpoint not found)\")\n",
    "            continue\n",
    "        \n",
    "        # \ucea1\uc158 \uc0dd\uc131\n",
    "        generated, _ = generate_caption_for_video(m, ip, video_path, CONFIG)\n",
    "        results[exp_name] = generated\n",
    "        print(f\"  Caption: {generated[:200]}...\" if len(generated) > 200 else f\"  Caption: {generated}\")\n",
    "        \n",
    "        # \uba54\ubaa8\ub9ac \uc815\ub9ac\n",
    "        del m\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    if gt_caption:\n",
    "        print(f\"\\n--- Ground Truth ---\")\n",
    "        print(f\"  {gt_caption[:200]}...\" if len(gt_caption) > 200 else f\"  {gt_caption}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# \uc2e4\ud589\ud558\ub824\uba74 \uc544\ub798 \uc8fc\uc11d \ud574\uc81c\n",
    "# results = compare_models(video_idx=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. \uc815\ub9ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU \uba54\ubaa8\ub9ac \uc815\ub9ac\n",
    "import gc\n",
    "\n",
    "if 'model' in dir():\n",
    "    del model\n",
    "if 'vision_encoder' in dir():\n",
    "    del vision_encoder\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\u2705 Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}