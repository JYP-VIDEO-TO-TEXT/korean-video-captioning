{"cells":[{"cell_type":"markdown","metadata":{"id":"IiISdGdyfxHZ"},"source":["# Person A-v2: Linear Projector (최적화 버전)\n","\n","**실험 목록**:\n","- E1-v2: Linear Projector + LoRA + 최적화\n","\n","**핵심 변경사항**:\n","- Linear 대신 **Linear Projector** 사용 (4M params)\n","- LR Scheduler (Warmup + Cosine Decay)\n","- Gradient Clipping (max_norm=1.0)\n","- Diversity Monitoring\n","- Vision Features 캐싱\n","\n","**목적**: 단순한 Projector가 Mode Collapse 없이 학습 가능한지 확인\n","\n","---\n","\n","**체크포인트 기반 재시작 지원**"]},{"cell_type":"markdown","metadata":{"id":"cFdZfSC4fxHb"},"source":["## 1. 환경 설정 (런타임 시작 시 항상 실행)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2436,"status":"ok","timestamp":1768970101504,"user":{"displayName":"Seoyeon Jo","userId":"16995728477064773929"},"user_tz":-540},"id":"_aU2zfyjfxHb","outputId":"ebba22f9-424c-4c1b-e5f6-47bf9326b142"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","============================================================\n","DATA_PATH:    /content/drive/MyDrive/mutsa-02/aihub_splitted\n","RESULTS_DIR:  /content/drive/MyDrive/mutsa-02/korean_video_captioning/siglip_study/results3\n","============================================================\n","Data found! Train: 5, Val: 4\n"]}],"source":["# Google Drive 마운트\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# ============================================\n","# 경로 설정\n","# ============================================\n","DRIVE_ROOT = \"/content/drive/MyDrive/mutsa-02\"\n","\n","# 데이터 경로 (aihub_splitted는 mutsa-02 바로 아래)\n","DATA_PATH = f\"{DRIVE_ROOT}/aihub_splitted\"\n","\n","# 결과 저장 경로 (siglip_study/results3) - v2/v3 실험용\n","RESULTS_DIR = f\"{DRIVE_ROOT}/korean_video_captioning/siglip_study/results3\"\n","\n","import os\n","os.makedirs(RESULTS_DIR, exist_ok=True)\n","\n","print(f\"{'='*60}\")\n","print(f\"DATA_PATH:    {DATA_PATH}\")\n","print(f\"RESULTS_DIR:  {RESULTS_DIR}\")\n","print(f\"{'='*60}\")\n","\n","if os.path.exists(DATA_PATH):\n","    train_n = len(os.listdir(f\"{DATA_PATH}/train\")) if os.path.exists(f\"{DATA_PATH}/train\") else 0\n","    val_n = len(os.listdir(f\"{DATA_PATH}/val\")) if os.path.exists(f\"{DATA_PATH}/val\") else 0\n","    print(f\"Data found! Train: {train_n}, Val: {val_n}\")\n","else:\n","    print(f\"WARNING: Data path not found!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5iYQho6vfxHb"},"outputs":[],"source":["!pip install -q transformers\u003e=4.40.0 accelerate bitsandbytes peft\n","!pip install -q torch torchvision\n","!pip install -q av decord opencv-python pillow\n","!pip install -q tqdm matplotlib pandas\n","!pip install -q evaluate bert_score nltk"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12966,"status":"ok","timestamp":1768970136455,"user":{"displayName":"Seoyeon Jo","userId":"16995728477064773929"},"user_tz":-540},"id":"cT2c5eMxfxHb","outputId":"d8bd941b-ee2e-4b2c-8cd9-309d67c35f54"},"outputs":[{"name":"stdout","output_type":"stream","text":["PyTorch: 2.9.0+cu126, CUDA: True\n","GPU: NVIDIA A100-SXM4-80GB (85.2GB)\n"]}],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from pathlib import Path\n","import json, os, random\n","import numpy as np\n","from PIL import Image\n","from tqdm import tqdm\n","import pandas as pd\n","from datetime import datetime\n","from typing import List, Dict, Optional\n","import cv2\n","\n","from transformers import (\n","    AutoModel, AutoProcessor, AutoModelForCausalLM, AutoTokenizer,\n","    BitsAndBytesConfig, CLIPVisionModel, CLIPImageProcessor,\n","    get_cosine_schedule_with_warmup,  # [OPT] LR Scheduler\n",")\n","from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n","\n","print(f\"PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}\")\n","if torch.cuda.is_available():\n","    print(f\"GPU: {torch.cuda.get_device_name(0)} ({torch.cuda.get_device_properties(0).total_memory/1e9:.1f}GB)\")"]},{"cell_type":"markdown","metadata":{"id":"cW-dVuGnfxHc"},"source":["## 2. 설정"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1768970136469,"user":{"displayName":"Seoyeon Jo","userId":"16995728477064773929"},"user_tz":-540},"id":"8NeSgHAIfxHc","outputId":"8cb92e8c-e121-45ea-d7e7-025bcbb03d59"},"outputs":[{"name":"stdout","output_type":"stream","text":["Config loaded (Linear Projector + Optimizations)\n","  Projector: Linear (~4M params)\n"]}],"source":["CONFIG = {\n","    \"vision_encoder\": \"openai/clip-vit-large-patch14-336\",\n","    \"llm\": \"Qwen/Qwen3-8B\",\n","    \"siglip_model\": \"google/siglip2-so400m-patch14-384\",\n","    \"lora_r\": 16, \"lora_alpha\": 32, \"lora_dropout\": 0.05,\n","\n","    # ============================================\n","    # [v2] Linear Projector 설정\n","    # ============================================\n","    \"stage1_epochs\": 2,\n","    \"stage1_lr\": 1e-3,\n","    \"stage2_epochs\": 3,\n","    \"stage2_lr\": 5e-5,\n","\n","    # ============================================\n","    # [OPT] 추가 최적화\n","    # ============================================\n","    \"warmup_ratio\": 0.1,\n","    \"max_grad_norm\": 1.0,\n","\n","    \"batch_size\": 4, \"gradient_accumulation\": 2,\n","    \"num_frames\": 8, \"max_length\": 768, \"seed\": 42,\n","    \"data_path\": DATA_PATH,\n","    \"results_dir\": RESULTS_DIR,\n","    \"prompt\": \"이 영상을 자세히 설명해주세요.\",\n","    \"max_new_tokens\": 128,\n","    \"repetition_penalty\": 1.2,\n","}\n","\n","def set_seed(seed):\n","    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n","    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n","set_seed(CONFIG[\"seed\"])\n","print(\"Config loaded (Linear Projector + Optimizations)\")\n","print(f\"  Projector: Linear (~4M params)\")"]},{"cell_type":"markdown","metadata":{"id":"1Rh3E3hsfxHc"},"source":["## 3. 핵심 클래스 정의"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1768970136487,"user":{"displayName":"Seoyeon Jo","userId":"16995728477064773929"},"user_tz":-540},"id":"Qk1gBA9GfxHc","outputId":"ae39d662-97d6-48d3-f5f6-3fa29e23891c"},"outputs":[{"name":"stdout","output_type":"stream","text":["SigLIPEvaluator defined.\n"]}],"source":["# SigLIP2 Evaluator (Multilingual - Korean supported)\n","class SigLIPEvaluator:\n","    def __init__(self, model_name=\"google/siglip2-so400m-patch14-384\", device=\"cuda\"):\n","        print(f\"Loading SigLIP2: {model_name}\")\n","        self.model = AutoModel.from_pretrained(model_name).to(device)\n","        self.processor = AutoProcessor.from_pretrained(model_name)\n","        self.model.eval(); self.device = device\n","\n","    @torch.no_grad()\n","    def compute_score(self, frames, caption):\n","        if not frames or not caption: return None\n","        try:\n","            inputs = self.processor(\n","                text=[caption], images=frames, return_tensors=\"pt\", padding=True,\n","                truncation=True, max_length=64\n","            ).to(self.device)\n","            return torch.sigmoid(self.model(**inputs).logits_per_image).mean().item()\n","        except Exception as e:\n","            print(f\"SigLIP error: {e}\")\n","            return None\n","\n","    def evaluate_batch(self, samples):\n","        scores, errors = [], 0\n","        for s in tqdm(samples, desc=\"SigLIP\"):\n","            score = self.compute_score(s[\"frames\"], s[\"caption\"])\n","            if score is not None: scores.append(score)\n","            else: errors += 1\n","        return {\n","            \"siglip_score\": np.mean(scores) if scores else 0.0,\n","            \"siglip_std\": np.std(scores) if scores else 0.0,\n","            \"num_samples\": len(scores), \"num_errors\": errors\n","        }\n","\n","print(\"SigLIPEvaluator defined.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1768970136491,"user":{"displayName":"Seoyeon Jo","userId":"16995728477064773929"},"user_tz":-540},"id":"yVohQyJPfxHc","outputId":"952d4945-da3b-4986-bce8-8c1153a8f2cc"},"outputs":[{"name":"stdout","output_type":"stream","text":["TextMetricsEvaluator defined.\n"]}],"source":["# Text Metrics Evaluator (METEOR, BERTScore)\n","class TextMetricsEvaluator:\n","    def __init__(self):\n","        import evaluate\n","        import nltk\n","        nltk.download('wordnet', quiet=True)\n","        nltk.download('punkt', quiet=True)\n","        nltk.download('omw-1.4', quiet=True)\n","        self.meteor = evaluate.load(\"meteor\")\n","        self.bertscore = evaluate.load(\"bertscore\")\n","        print(\"TextMetricsEvaluator ready\")\n","\n","    def compute_scores(self, predictions: list, references: list) -\u003e dict:\n","        if not predictions or not references:\n","            return {\"meteor\": 0.0, \"bertscore_f1\": 0.0}\n","        meteor_result = self.meteor.compute(predictions=predictions, references=references)\n","        bert_result = self.bertscore.compute(predictions=predictions, references=references, lang=\"ko\")\n","        return {\n","            \"meteor\": meteor_result[\"meteor\"],\n","            \"bertscore_precision\": np.mean(bert_result[\"precision\"]),\n","            \"bertscore_recall\": np.mean(bert_result[\"recall\"]),\n","            \"bertscore_f1\": np.mean(bert_result[\"f1\"]),\n","        }\n","\n","print(\"TextMetricsEvaluator defined.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1768970136517,"user":{"displayName":"Seoyeon Jo","userId":"16995728477064773929"},"user_tz":-540},"id":"eGAWrVMefxHc","outputId":"6463db6e-1b79-4ff7-eb38-ec9caec7f1d5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Linear Projector params: 4,198,400\n"]}],"source":["# Linear Projector\n","class LinearProjector(nn.Module):\n","    \"\"\"단순 선형 변환 Projector (약 4M params)\"\"\"\n","    def __init__(self, vision_dim=1024, llm_dim=4096):\n","        super().__init__()\n","        self.proj = nn.Linear(vision_dim, llm_dim)\n","\n","    def forward(self, x):\n","        return self.proj(x)\n","\n","def create_projector(projector_type, vision_dim=1024, llm_dim=4096, config=None):\n","    if projector_type == \"linear\":\n","        return LinearProjector(vision_dim, llm_dim)\n","    raise ValueError(f\"Unknown: {projector_type}\")\n","\n","print(f\"Linear Projector params: {sum(p.numel() for p in LinearProjector().parameters()):,}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1768970136727,"user":{"displayName":"Seoyeon Jo","userId":"16995728477064773929"},"user_tz":-540},"id":"Lb8I5rTBfxHc","outputId":"236c3923-6f5c-4d58-c0a7-337322b686df"},"outputs":[{"name":"stdout","output_type":"stream","text":["CustomVLM defined (with cache support).\n"]}],"source":["# Custom VLM (Vision Features 캐싱 지원)\n","class CustomVLM(nn.Module):\n","    def __init__(self, vision_encoder, projector, llm, tokenizer):\n","        super().__init__()\n","        self.vision_encoder = vision_encoder\n","        self.projector = projector\n","        self.llm = llm\n","        self.tokenizer = tokenizer\n","        for p in self.vision_encoder.parameters(): p.requires_grad = False\n","        self.vision_encoder.eval()\n","\n","    def encode_video(self, frames):\n","        with torch.no_grad():\n","            features = self.vision_encoder(pixel_values=frames).last_hidden_state[:, 1:, :]\n","        return features.reshape(-1, features.size(-1))\n","\n","    def forward_with_cache(self, vision_features, input_ids, attention_mask, labels=None):\n","        \"\"\"[OPT] 캐싱된 Vision Features로 forward (Vision Encoder 스킵)\"\"\"\n","        batch_size, device = vision_features.size(0), vision_features.device\n","        all_vision = [self.projector(vision_features[i]) for i in range(batch_size)]\n","        text_embeds = self.llm.get_input_embeddings()(input_ids)\n","\n","        combined_e, combined_a, combined_l = [], [], []\n","        for i in range(batch_size):\n","            v_len = all_vision[i].size(0)\n","            combined_e.append(torch.cat([all_vision[i], text_embeds[i]], dim=0))\n","            combined_a.append(torch.cat([torch.ones(v_len, device=device), attention_mask[i]], dim=0))\n","            if labels is not None:\n","                combined_l.append(torch.cat([torch.full((v_len,), -100, device=device, dtype=labels.dtype), labels[i]], dim=0))\n","\n","        max_len = max(e.size(0) for e in combined_e)\n","        pad_e = torch.zeros(batch_size, max_len, combined_e[0].size(-1), device=device)\n","        pad_a = torch.zeros(batch_size, max_len, device=device)\n","        pad_l = torch.full((batch_size, max_len), -100, device=device, dtype=torch.long) if labels is not None else None\n","\n","        for i in range(batch_size):\n","            sl = combined_e[i].size(0)\n","            pad_e[i, :sl], pad_a[i, :sl] = combined_e[i], combined_a[i]\n","            if labels is not None: pad_l[i, :sl] = combined_l[i]\n","\n","        return self.llm(inputs_embeds=pad_e, attention_mask=pad_a, labels=pad_l, return_dict=True)\n","\n","    def forward(self, frames, input_ids, attention_mask, labels=None):\n","        batch_size, device = frames.size(0), frames.device\n","        all_vision = [self.projector(self.encode_video(frames[i])) for i in range(batch_size)]\n","        text_embeds = self.llm.get_input_embeddings()(input_ids)\n","\n","        combined_e, combined_a, combined_l = [], [], []\n","        for i in range(batch_size):\n","            v_len = all_vision[i].size(0)\n","            combined_e.append(torch.cat([all_vision[i], text_embeds[i]], dim=0))\n","            combined_a.append(torch.cat([torch.ones(v_len, device=device), attention_mask[i]], dim=0))\n","            if labels is not None:\n","                combined_l.append(torch.cat([torch.full((v_len,), -100, device=device, dtype=labels.dtype), labels[i]], dim=0))\n","\n","        max_len = max(e.size(0) for e in combined_e)\n","        pad_e = torch.zeros(batch_size, max_len, combined_e[0].size(-1), device=device)\n","        pad_a = torch.zeros(batch_size, max_len, device=device)\n","        pad_l = torch.full((batch_size, max_len), -100, device=device, dtype=torch.long) if labels is not None else None\n","\n","        for i in range(batch_size):\n","            sl = combined_e[i].size(0)\n","            pad_e[i, :sl], pad_a[i, :sl] = combined_e[i], combined_a[i]\n","            if labels is not None: pad_l[i, :sl] = combined_l[i]\n","\n","        return self.llm(inputs_embeds=pad_e, attention_mask=pad_a, labels=pad_l, return_dict=True)\n","\n","    @torch.no_grad()\n","    def generate(self, frames, prompt, max_new_tokens=128):\n","        device = frames.device\n","        vision_embeds = self.projector(self.encode_video(frames))\n","        text_inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(device)\n","        text_embeds = self.llm.get_input_embeddings()(text_inputs.input_ids)\n","        combined = torch.cat([vision_embeds.unsqueeze(0), text_embeds], dim=1)\n","        outputs = self.llm.generate(\n","            inputs_embeds=combined, max_new_tokens=max_new_tokens, do_sample=False,\n","            repetition_penalty=1.2,\n","            pad_token_id=self.tokenizer.pad_token_id, eos_token_id=self.tokenizer.eos_token_id\n","        )\n","        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","print(\"CustomVLM defined (with cache support).\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1768970136753,"user":{"displayName":"Seoyeon Jo","userId":"16995728477064773929"},"user_tz":-540},"id":"ySBlxKV6fxHd","outputId":"4788624e-ddc7-4350-a968-e6f679be5352"},"outputs":[{"name":"stdout","output_type":"stream","text":["CachedVideoDataset defined.\n"]}],"source":["# Dataset with Vision Features Caching\n","def extract_frames(video_path, num_frames=8):\n","    cap = cv2.VideoCapture(video_path)\n","    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    if total \u003c= 0: cap.release(); return []\n","    frames = []\n","    for idx in np.linspace(0, total-1, num_frames, dtype=int):\n","        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n","        ret, frame = cap.read()\n","        if ret: frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n","    cap.release()\n","    return frames\n","\n","class CachedVideoDataset(Dataset):\n","    \"\"\"[OPT] Vision Features를 미리 계산하여 캐싱하는 Dataset.\n","\n","    Vision Encoder는 frozen이므로 같은 이미지에 대해 항상 같은 출력.\n","    한 번 계산해서 저장하면 매 epoch마다 Vision Encoder 추론 시간 절약.\n","    \"\"\"\n","\n","    def __init__(self, data_path, split, vision_encoder, image_processor, tokenizer,\n","                 num_frames=8, max_length=512, max_samples=None, prompt=\"이 영상을 자세히 설명해주세요.\", device=\"cuda\"):\n","        self.data_path, self.split = Path(data_path), split\n","        self.image_processor, self.tokenizer = image_processor, tokenizer\n","        self.num_frames, self.max_length = num_frames, max_length\n","        self.prompt = prompt\n","        self.device = device\n","        self.samples = self._load(max_samples)\n","        self.prompt_len = len(tokenizer(prompt, add_special_tokens=False).input_ids)\n","\n","        # Vision Features 캐싱\n","        self.vision_cache = {}\n","        self.pil_cache = {}  # 평가용 PIL 이미지\n","        print(f\"\\n[CACHE] Pre-computing vision features for {len(self.samples)} {split} samples...\")\n","        vision_encoder.eval()\n","        with torch.no_grad():\n","            for idx in tqdm(range(len(self.samples)), desc=f\"Caching {split}\"):\n","                s = self.samples[idx]\n","                frames = extract_frames(s[\"video_path\"], self.num_frames)\n","                if not frames:\n","                    frames = [np.zeros((336, 336, 3), dtype=np.uint8)] * self.num_frames\n","                while len(frames) \u003c self.num_frames: frames.append(frames[-1].copy())\n","                frames = frames[:self.num_frames]\n","\n","                pil_frames = [Image.fromarray(f) for f in frames]\n","                self.pil_cache[idx] = pil_frames\n","\n","                pixel_values = self.image_processor(images=pil_frames, return_tensors=\"pt\").pixel_values.to(device)\n","                features = vision_encoder(pixel_values=pixel_values).last_hidden_state[:, 1:, :]\n","                # CPU로 이동하여 GPU 메모리 절약\n","                self.vision_cache[idx] = features.reshape(-1, features.size(-1)).cpu()\n","\n","        print(f\"[CACHE] Done! Cached {len(self.vision_cache)} samples.\")\n","        print(f\"[CACHE] Feature shape: {self.vision_cache[0].shape}\")\n","\n","    def _load(self, max_samples):\n","        samples = []\n","        label_dir = self.data_path / self.split / \"labels\"\n","        video_dir = self.data_path / self.split / \"videos\"\n","        if not label_dir.exists(): return samples\n","        for lf in sorted(label_dir.glob(\"*.json\")):\n","            try:\n","                with open(lf, \"r\", encoding=\"utf-8\") as f: label = json.load(f)\n","                vp = video_dir / (lf.stem + \".mp4\")\n","                if vp.exists() and (cap := label.get(\"annotation\", {}).get(\"description_kr\", \"\")):\n","                    samples.append({\"video_path\": str(vp), \"caption\": cap})\n","            except: pass\n","            if max_samples and len(samples) \u003e= max_samples: break\n","        return samples\n","\n","    def __len__(self): return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        s = self.samples[idx]\n","        vision_features = self.vision_cache[idx]  # 캐싱된 features 사용\n","        pil_frames = self.pil_cache[idx]\n","\n","        full_text = f\"{self.prompt} {s['caption']}\"\n","        ti = self.tokenizer(full_text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n","\n","        return {\n","            \"vision_features\": vision_features,\n","            \"input_ids\": ti.input_ids.squeeze(0),\n","            \"attention_mask\": ti.attention_mask.squeeze(0),\n","            \"caption\": s[\"caption\"],\n","            \"pil_frames\": pil_frames,\n","            \"prompt_len\": self.prompt_len\n","        }\n","\n","def create_cached_collate_fn(pad_token_id):\n","    def collate_fn(batch):\n","        labels = []\n","        for b in batch:\n","            label = b[\"input_ids\"].clone()\n","            label[label == pad_token_id] = -100\n","            if \"prompt_len\" in b:\n","                label[:b[\"prompt_len\"]] = -100\n","            labels.append(label)\n","        return {\n","            \"vision_features\": torch.stack([b[\"vision_features\"] for b in batch]),\n","            \"input_ids\": torch.stack([b[\"input_ids\"] for b in batch]),\n","            \"attention_mask\": torch.stack([b[\"attention_mask\"] for b in batch]),\n","            \"labels\": torch.stack(labels),\n","            \"captions\": [b[\"caption\"] for b in batch],\n","            \"pil_frames\": [b[\"pil_frames\"] for b in batch],\n","        }\n","    return collate_fn\n","\n","print(\"CachedVideoDataset defined.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1768970136756,"user":{"displayName":"Seoyeon Jo","userId":"16995728477064773929"},"user_tz":-540},"id":"3INpQqQvfxHd","outputId":"4f47ddf4-6231-45ff-9038-df67b2f278d4"},"outputs":[{"name":"stdout","output_type":"stream","text":["CheckpointManager defined.\n"]}],"source":["# Checkpoint Manager\n","class CheckpointManager:\n","    def __init__(self, exp_name, results_dir):\n","        self.exp_dir = Path(results_dir) / exp_name\n","        self.exp_dir.mkdir(parents=True, exist_ok=True)\n","        self.ckpt_dir = self.exp_dir / \"checkpoints\"\n","        self.ckpt_dir.mkdir(exist_ok=True)\n","        self.logs = []\n","        self.best_score = 0.0\n","        log_file = self.exp_dir / \"training_log.csv\"\n","        if log_file.exists(): self.logs = pd.read_csv(log_file).to_dict('records')\n","        print(f\"CheckpointManager: {self.exp_dir}\")\n","\n","    def is_completed(self): return (self.exp_dir / \"final_metrics.json\").exists()\n","\n","    def get_resume_info(self):\n","        for e in range(10, 0, -1):\n","            p = self.ckpt_dir / f\"stage2_epoch{e}_checkpoint.pt\"\n","            if p.exists(): return {\"stage\": 2, \"epoch\": e, \"path\": p}\n","        p = self.ckpt_dir / \"stage1_checkpoint.pt\"\n","        if p.exists(): return {\"stage\": 1, \"epoch\": \"done\", \"path\": p}\n","        return None\n","\n","    def log(self, m):\n","        m[\"timestamp\"] = datetime.now().isoformat()\n","        self.logs.append(m)\n","        pd.DataFrame(self.logs).to_csv(self.exp_dir / \"training_log.csv\", index=False)\n","\n","    def save_checkpoint(self, model, stage, epoch, metrics, optimizer=None, scheduler=None):\n","        ckpt = {\"projector_state_dict\": model.projector.state_dict(), \"metrics\": metrics, \"stage\": stage, \"epoch\": epoch}\n","        if optimizer: ckpt[\"optimizer_state_dict\"] = optimizer.state_dict()\n","        if scheduler: ckpt[\"scheduler_state_dict\"] = scheduler.state_dict()\n","        path = self.ckpt_dir / (\"stage1_checkpoint.pt\" if stage == 1 else f\"stage2_epoch{epoch}_checkpoint.pt\")\n","        torch.save(ckpt, path)\n","        print(f\"Saved: {path}\")\n","        if (s := metrics.get(\"siglip_score\", 0)) \u003e self.best_score:\n","            self.best_score = s\n","            torch.save(ckpt, self.ckpt_dir / \"best_model.pt\")\n","            print(f\"New best! Score: {s:.4f}\")\n","\n","    def save_final(self, m):\n","        with open(self.exp_dir / \"final_metrics.json\", \"w\") as f: json.dump(m, f, indent=2)\n","        print(\"Experiment completed!\")\n","\n","print(\"CheckpointManager defined.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1768970136759,"user":{"displayName":"Seoyeon Jo","userId":"16995728477064773929"},"user_tz":-540},"id":"121DgE2efxHd","outputId":"381e6f98-721d-4201-800f-180c6b083a94"},"outputs":[{"name":"stdout","output_type":"stream","text":["Helpers defined.\n"]}],"source":["# Logging helpers\n","import time\n","\n","def get_gpu_memory():\n","    if torch.cuda.is_available():\n","        return {\"allocated\": torch.cuda.memory_allocated()/1e9, \"total\": torch.cuda.get_device_properties(0).total_memory/1e9}\n","    return {\"allocated\": 0, \"total\": 0}\n","\n","def format_time(s):\n","    h, m, sec = int(s//3600), int((s%3600)//60), int(s%60)\n","    return f\"{h}h{m}m{sec}s\" if h else (f\"{m}m{sec}s\" if m else f\"{sec}s\")\n","\n","def print_config(config, proj_type, train_n, val_n):\n","    mem = get_gpu_memory()\n","    print(f\"\\n{'-'*60}\\nCONFIG: {proj_type} | Train: {train_n} | Val: {val_n}\")\n","    print(f\"Batch: {config['batch_size']}x{config['gradient_accumulation']} | Frames: {config['num_frames']} | MaxLen: {config['max_length']}\")\n","    print(f\"Stage1: {config['stage1_epochs']}ep (LR:{config['stage1_lr']}) | Stage2: {config['stage2_epochs']}ep (LR:{config['stage2_lr']})\")\n","    print(f\"Warmup: {config['warmup_ratio']*100:.0f}% | Grad Clip: {config['max_grad_norm']}\")\n","    print(f\"GPU: {mem['allocated']:.1f}GB / {mem['total']:.1f}GB\\n{'-'*60}\")\n","\n","def compute_diversity(captions):\n","    \"\"\"[OPT] 캡션 다양성 계산 (0.0 ~ 1.0). Mode Collapse 탐지용.\"\"\"\n","    if not captions: return 0.0\n","    unique = len(set(captions))\n","    return unique / len(captions)\n","\n","print(\"Helpers defined.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38,"status":"ok","timestamp":1768970136797,"user":{"displayName":"Seoyeon Jo","userId":"16995728477064773929"},"user_tz":-540},"id":"k2tgLyIcfxHd","outputId":"05bbb4d8-33de-4ab2-988d-3042e864234a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training functions defined (with optimizations).\n"]}],"source":["# Training functions with optimizations\n","from torch.amp import autocast, GradScaler\n","\n","def evaluate_model(model, val_loader, siglip_evaluator, prompt, device, max_samples=None, text_evaluator=None):\n","    \"\"\"검증 평가 + Diversity 모니터링.\"\"\"\n","    model.eval()\n","    samples, num_eval = [], 0\n","    predictions, references = [], []\n","    total = len(val_loader.dataset) if not max_samples else min(max_samples, len(val_loader.dataset))\n","    print(f\"\\n[Eval] Generating {total} captions...\")\n","    start = time.time()\n","\n","    cap_lens, empty_cnt, rep_scores = [], 0, []\n","    all_captions = []  # [OPT] Diversity 계산용\n","\n","    with torch.no_grad():\n","        for batch in tqdm(val_loader, desc=\"Eval\"):\n","            if max_samples and num_eval \u003e= max_samples: break\n","            # 캐싱된 데이터의 경우 pixel_values 대신 vision_features 사용 (평가는 원본 사용)\n","            if \"pixel_values\" in batch:\n","                pv = batch[\"pixel_values\"].to(device)\n","            else:\n","                # 평가 시에는 PIL에서 다시 처리\n","                pv = None\n","            gt_captions = batch[\"captions\"]\n","            pil_frames_batch = batch[\"pil_frames\"]\n","\n","            for i in range(len(gt_captions)):\n","                if max_samples and num_eval \u003e= max_samples: break\n","\n","                # 캐싱된 경우 PIL에서 다시 처리\n","                if pv is None:\n","                    from transformers import CLIPImageProcessor\n","                    img_proc = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n","                    frames_tensor = img_proc(images=pil_frames_batch[i], return_tensors=\"pt\").pixel_values.to(device)\n","                else:\n","                    frames_tensor = pv[i]\n","\n","                cap = model.generate(frames_tensor, prompt)\n","\n","                cap_lens.append(len(cap))\n","                if not cap.strip(): empty_cnt += 1\n","                tokens = cap.split()\n","                if len(tokens) \u003e 1: rep_scores.append(len(set(tokens))/len(tokens))\n","                all_captions.append(cap)\n","\n","                if num_eval \u003c 3:\n","                    print(f\"\\n  [Sample {num_eval+1}]\")\n","                    print(f\"    Gen: {cap[:100]}{'...' if len(cap)\u003e100 else ''}\")\n","                    print(f\"    GT:  {gt_captions[i][:100]}{'...' if len(gt_captions[i])\u003e100 else ''}\")\n","\n","                samples.append({\"frames\": pil_frames_batch[i], \"caption\": cap})\n","                predictions.append(cap)\n","                references.append(gt_captions[i])\n","                num_eval += 1\n","\n","    # [OPT] Diversity 계산\n","    diversity = compute_diversity(all_captions)\n","    print(f\"\\n[Eval] Done in {format_time(time.time()-start)}\")\n","    print(f\"[DEBUG] Empty:{empty_cnt}, Diversity:{diversity:.2f} ({len(set(all_captions))}/{len(all_captions)} unique)\")\n","\n","    # [OPT] Mode Collapse 경고\n","    if diversity \u003c 0.5:\n","        print(f\"\\n\" + \"!\"*60)\n","        print(f\"  WARNING: Low diversity ({diversity:.2f}) - POSSIBLE MODE COLLAPSE!\")\n","        print(f\"!\"*60 + \"\\n\")\n","\n","    print(\"[Eval] Computing SigLIP2...\")\n","    metrics = siglip_evaluator.evaluate_batch(samples)\n","    metrics[\"empty_count\"] = empty_cnt\n","    metrics[\"diversity\"] = diversity  # [OPT] 추가\n","\n","    if text_evaluator is not None:\n","        print(\"[Eval] Computing METEOR/BERTScore...\")\n","        text_metrics = text_evaluator.compute_scores(predictions, references)\n","        metrics.update(text_metrics)\n","        print(f\"  METEOR: {text_metrics['meteor']:.4f}, BERTScore-F1: {text_metrics['bertscore_f1']:.4f}\")\n","\n","    model.train()\n","    return metrics\n","\n","def train_stage1(model, train_loader, config, ckpt_mgr, device):\n","    \"\"\"[OPT] LR Scheduler + Gradient Clipping 적용.\"\"\"\n","    print(\"\\n\" + \"=\"*60 + \"\\n  STAGE 1: Linear Warm-up (with LR Scheduler)\\n\" + \"=\"*60)\n","    for p in model.llm.parameters(): p.requires_grad = False\n","    for p in model.projector.parameters(): p.requires_grad = True\n","\n","    proj_params = sum(p.numel() for p in model.projector.parameters() if p.requires_grad)\n","    total_steps = len(train_loader) * config[\"stage1_epochs\"]\n","    warmup_steps = int(config[\"warmup_ratio\"] * total_steps)\n","\n","    print(f\"  Trainable: {proj_params:,} params\")\n","    print(f\"  Total Steps: {total_steps} | Warmup: {warmup_steps} ({config['warmup_ratio']*100:.0f}%)\")\n","    print(f\"  LR: {config['stage1_lr']} | Grad Clip: {config['max_grad_norm']}\")\n","\n","    opt = torch.optim.AdamW(model.projector.parameters(), lr=config[\"stage1_lr\"])\n","    scheduler = get_cosine_schedule_with_warmup(opt, warmup_steps, total_steps)\n","    scaler = GradScaler('cuda')\n","    stage_start = time.time()\n","\n","    for epoch in range(config[\"stage1_epochs\"]):\n","        epoch_start = time.time()\n","        model.train(); total_loss = 0\n","        for i, batch in enumerate(pbar := tqdm(train_loader, desc=f\"S1 E{epoch+1}/{config['stage1_epochs']}\")):\n","            with autocast('cuda', dtype=torch.bfloat16):\n","                # [OPT] 캐싱된 features 사용\n","                loss = model.forward_with_cache(\n","                    batch[\"vision_features\"].to(device),\n","                    batch[\"input_ids\"].to(device),\n","                    batch[\"attention_mask\"].to(device),\n","                    batch[\"labels\"].to(device)\n","                ).loss\n","\n","            scaler.scale(loss).backward()\n","\n","            if (i+1) % config[\"gradient_accumulation\"] == 0:\n","                # [OPT] Gradient Clipping\n","                scaler.unscale_(opt)\n","                torch.nn.utils.clip_grad_norm_(model.projector.parameters(), config[\"max_grad_norm\"])\n","                scaler.step(opt)\n","                scaler.update()\n","                scheduler.step()  # [OPT] LR Scheduler\n","                opt.zero_grad()\n","\n","            total_loss += loss.item()\n","            current_lr = scheduler.get_last_lr()[0]\n","            mem = get_gpu_memory()\n","            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\", \"lr\": f\"{current_lr:.2e}\", \"mem\": f\"{mem['allocated']:.1f}G\"})\n","\n","        avg = total_loss / len(train_loader)\n","        print(f\"\\n[S1 E{epoch+1}] Loss: {avg:.4f} | LR: {current_lr:.2e} | Time: {format_time(time.time()-epoch_start)}\")\n","        ckpt_mgr.log({\"stage\": 1, \"epoch\": epoch+1, \"train_loss\": avg, \"lr\": current_lr})\n","\n","    print(f\"\\n{'='*60}\\n  Stage 1 Complete! Time: {format_time(time.time()-stage_start)} | Loss: {avg:.4f}\\n{'='*60}\")\n","    ckpt_mgr.save_checkpoint(model, 1, config[\"stage1_epochs\"], {\"train_loss\": avg}, opt, scheduler)\n","\n","def train_stage2(model, train_loader, val_loader, siglip_evaluator, config, ckpt_mgr, device, start_epoch=0, text_evaluator=None):\n","    \"\"\"[OPT] LR Scheduler + Gradient Clipping + Diversity 모니터링.\"\"\"\n","    remain = config[\"stage2_epochs\"] - start_epoch\n","    print(f\"\\n\" + \"=\"*60 + f\"\\n  STAGE 2: Linear + LoRA (E{start_epoch+1}-{config['stage2_epochs']})\\n\" + \"=\"*60)\n","\n","    for p in model.projector.parameters(): p.requires_grad = True\n","    for name, param in model.llm.named_parameters():\n","        if 'lora' in name.lower(): param.requires_grad = True\n","\n","    proj_p = sum(p.numel() for p in model.projector.parameters() if p.requires_grad)\n","    lora_p = sum(p.numel() for p in model.llm.parameters() if p.requires_grad)\n","    total_steps = len(train_loader) * remain\n","    warmup_steps = int(config[\"warmup_ratio\"] * total_steps)\n","\n","    print(f\"  Trainable: Proj {proj_p:,} + LoRA {lora_p:,} = {proj_p+lora_p:,}\")\n","    print(f\"  Total Steps: {total_steps} | Warmup: {warmup_steps}\")\n","    print(f\"  LR: {config['stage2_lr']} | Grad Clip: {config['max_grad_norm']}\")\n","\n","    all_params = list(model.projector.parameters()) + [p for p in model.llm.parameters() if p.requires_grad]\n","    opt = torch.optim.AdamW(all_params, lr=config[\"stage2_lr\"])\n","    scheduler = get_cosine_schedule_with_warmup(opt, warmup_steps, total_steps)\n","    scaler = GradScaler('cuda')\n","\n","    stage_start = time.time()\n","    best = ckpt_mgr.best_score\n","\n","    for epoch in range(start_epoch, config[\"stage2_epochs\"]):\n","        epoch_start = time.time()\n","        model.train(); total_loss = 0\n","\n","        for i, batch in enumerate(pbar := tqdm(train_loader, desc=f\"S2 E{epoch+1}/{config['stage2_epochs']}\")):\n","            with autocast('cuda', dtype=torch.bfloat16):\n","                loss = model.forward_with_cache(\n","                    batch[\"vision_features\"].to(device),\n","                    batch[\"input_ids\"].to(device),\n","                    batch[\"attention_mask\"].to(device),\n","                    batch[\"labels\"].to(device)\n","                ).loss\n","\n","            scaler.scale(loss).backward()\n","\n","            if (i+1) % config[\"gradient_accumulation\"] == 0:\n","                scaler.unscale_(opt)\n","                torch.nn.utils.clip_grad_norm_(all_params, config[\"max_grad_norm\"])\n","                scaler.step(opt)\n","                scaler.update()\n","                scheduler.step()\n","                opt.zero_grad()\n","\n","            total_loss += loss.item()\n","            current_lr = scheduler.get_last_lr()[0]\n","            mem = get_gpu_memory()\n","            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\", \"lr\": f\"{current_lr:.2e}\", \"mem\": f\"{mem['allocated']:.1f}G\"})\n","\n","        avg = total_loss / len(train_loader)\n","        print(f\"\\n[S2 E{epoch+1}] Train Loss: {avg:.4f} | LR: {current_lr:.2e} | Time: {format_time(time.time()-epoch_start)}\")\n","\n","        # Evaluation\n","        metrics = evaluate_model(model, val_loader, siglip_evaluator, config[\"prompt\"], device, text_evaluator=text_evaluator)\n","        is_best = metrics['siglip_score'] \u003e best\n","        if is_best: best = metrics['siglip_score']\n","\n","        print(f\"[S2 E{epoch+1}] Val SigLIP: {metrics['siglip_score']:.4f} | Diversity: {metrics.get('diversity', 0):.2f} {'(BEST!)' if is_best else ''} | Best: {best:.4f}\")\n","        ckpt_mgr.log({\"stage\": 2, \"epoch\": epoch+1, \"train_loss\": avg, \"siglip_score\": metrics[\"siglip_score\"], \"diversity\": metrics.get(\"diversity\", 0), \"lr\": current_lr})\n","        ckpt_mgr.save_checkpoint(model, 2, epoch+1, metrics, opt, scheduler)\n","\n","print(\"Training functions defined (with optimizations).\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1768970136800,"user":{"displayName":"Seoyeon Jo","userId":"16995728477064773929"},"user_tz":-540},"id":"SaCX9zV8fxHd","outputId":"74323cfc-adb5-4a50-87cc-9d0006eb68c3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Build function defined.\n"]}],"source":["# Model build\n","def build_model(config, projector_type, device, resume_path=None):\n","    print(f\"\\nBuilding {projector_type} model...\")\n","    vision_encoder = CLIPVisionModel.from_pretrained(config[\"vision_encoder\"]).to(device)\n","    image_processor = CLIPImageProcessor.from_pretrained(config[\"vision_encoder\"])\n","    vision_encoder.eval()\n","\n","    bnb = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True)\n","    llm = AutoModelForCausalLM.from_pretrained(config[\"llm\"], quantization_config=bnb, device_map=\"auto\", trust_remote_code=True)\n","    tokenizer = AutoTokenizer.from_pretrained(config[\"llm\"], trust_remote_code=True)\n","    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n","    print(f\"  PAD: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n","    print(f\"  EOS: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n","\n","    projector = create_projector(projector_type, vision_encoder.config.hidden_size, llm.config.hidden_size, config).to(device)\n","\n","    llm = prepare_model_for_kbit_training(llm)\n","    lora_cfg = LoraConfig(r=config[\"lora_r\"], lora_alpha=config[\"lora_alpha\"], lora_dropout=config[\"lora_dropout\"],\n","                          target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n","                          task_type=TaskType.CAUSAL_LM, bias=\"none\")\n","    llm = get_peft_model(llm, lora_cfg)\n","    llm.print_trainable_parameters()\n","    llm.config.use_cache = False  # type: ignore[attr-defined]\n","    llm.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n","\n","    model = CustomVLM(vision_encoder, projector, llm, tokenizer)\n","    if resume_path:\n","        print(f\"Loading checkpoint: {resume_path}\")\n","        ckpt = torch.load(resume_path, map_location=device)\n","        model.projector.load_state_dict(ckpt[\"projector_state_dict\"])\n","        print(f\"Resumed from stage {ckpt['stage']}, epoch {ckpt['epoch']}\")\n","    print(\"Model built!\")\n","    return model, vision_encoder, image_processor, tokenizer\n","\n","print(\"Build function defined.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1768970136819,"user":{"displayName":"Seoyeon Jo","userId":"16995728477064773929"},"user_tz":-540},"id":"3Nm-zAx0fxHd","outputId":"08456df1-e957-4f33-b6a6-2f627cdfc80b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Experiment runner defined.\n"]}],"source":["# Experiment runner (with Vision Caching)\n","def run_experiment(exp_name, projector_type, config):\n","    exp_start = time.time()\n","    print(\"\\n\" + \"#\"*60)\n","    print(f\"#  EXPERIMENT: {exp_name}\")\n","    print(f\"#  Projector: {projector_type}\")\n","    print(f\"#  LR: stage1={config['stage1_lr']}, stage2={config['stage2_lr']}\")\n","    print(f\"#  Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n","    print(\"#\"*60)\n","\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    ckpt_mgr = CheckpointManager(exp_name, config[\"results_dir\"])\n","\n","    if ckpt_mgr.is_completed():\n","        print(f\"\\n[SKIP] Already completed!\")\n","        with open(ckpt_mgr.exp_dir / \"final_metrics.json\") as f:\n","            r = json.load(f)\n","        print(f\"  Previous Score: {r.get('siglip_score', 'N/A')}\")\n","        return r\n","\n","    resume_info = ckpt_mgr.get_resume_info()\n","    resume_path = resume_info[\"path\"] if resume_info else None\n","    if resume_info:\n","        print(f\"\\n[RESUME] Stage {resume_info['stage']}, Epoch {resume_info['epoch']}\")\n","    else:\n","        print(f\"\\n[START] Starting from scratch\")\n","\n","    try:\n","        print(f\"\\n[1/5] Building model...\")\n","        model, vision_encoder, img_proc, tok = build_model(config, projector_type, device, resume_path)\n","\n","        print(f\"\\n[2/5] Loading datasets with Vision Caching...\")\n","        train_ds = CachedVideoDataset(\n","            config[\"data_path\"], \"train\", vision_encoder, img_proc, tok,\n","            config[\"num_frames\"], config[\"max_length\"], None, config[\"prompt\"], device\n","        )\n","        val_ds = CachedVideoDataset(\n","            config[\"data_path\"], \"val\", vision_encoder, img_proc, tok,\n","            config[\"num_frames\"], config[\"max_length\"], None, config[\"prompt\"], device\n","        )\n","        train_loader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True,\n","                                  collate_fn=create_cached_collate_fn(tok.pad_token_id),\n","                                  num_workers=4, pin_memory=True, persistent_workers=True)\n","        val_loader = DataLoader(val_ds, batch_size=config[\"batch_size\"], shuffle=False,\n","                                collate_fn=create_cached_collate_fn(tok.pad_token_id),\n","                                num_workers=4, pin_memory=True, persistent_workers=True)\n","        print_config(config, projector_type, len(train_ds), len(val_ds))\n","\n","        print(f\"[3/5] Loading SigLIP2 evaluator...\")\n","        siglip_evaluator = SigLIPEvaluator(config[\"siglip_model\"], device)\n","\n","        print(f\"[4/5] Loading Text Metrics evaluator...\")\n","        text_evaluator = TextMetricsEvaluator()\n","\n","        print(f\"\\n[5/5] Starting training...\")\n","        if resume_info is None:\n","            train_stage1(model, train_loader, config, ckpt_mgr, device)\n","            train_stage2(model, train_loader, val_loader, siglip_evaluator, config, ckpt_mgr, device, 0, None)\n","        elif resume_info[\"stage\"] == 1:\n","            train_stage2(model, train_loader, val_loader, siglip_evaluator, config, ckpt_mgr, device, 0, None)\n","        else:\n","            train_stage2(model, train_loader, val_loader, siglip_evaluator, config, ckpt_mgr, device, resume_info[\"epoch\"], None)\n","\n","        print(f\"\\n[FINAL] Running final evaluation...\")\n","        final = evaluate_model(model, val_loader, siglip_evaluator, config[\"prompt\"], device, None, text_evaluator)\n","        final[\"experiment\"], final[\"projector\"] = exp_name, projector_type\n","        final[\"config\"] = {\"stage1_lr\": config[\"stage1_lr\"], \"stage2_lr\": config[\"stage2_lr\"]}\n","        ckpt_mgr.save_final(final)\n","\n","        exp_time = time.time() - exp_start\n","        print(f\"\\n\" + \"=\"*60)\n","        print(f\"  EXPERIMENT COMPLETE: {exp_name}\")\n","        print(f\"=\"*60)\n","        print(f\"  Final SigLIP2: {final['siglip_score']:.4f}\")\n","        print(f\"  Final Diversity: {final.get('diversity', 0):.2f}\")\n","        if 'meteor' in final:\n","            print(f\"  Final METEOR: {final['meteor']:.4f}\")\n","            print(f\"  Final BERTScore-F1: {final['bertscore_f1']:.4f}\")\n","        print(f\"  Total Time: {format_time(exp_time)}\")\n","        print(f\"  Saved to: {ckpt_mgr.exp_dir}\")\n","        print(\"=\"*60)\n","\n","        del model, siglip_evaluator, text_evaluator; torch.cuda.empty_cache()\n","        return final\n","    except Exception as e:\n","        print(f\"\\n{'!'*60}\\n  ERROR in {exp_name}: {e}\\n{'!'*60}\")\n","        import traceback; traceback.print_exc()\n","        return {\"experiment\": exp_name, \"error\": str(e)}\n","\n","print(\"Experiment runner defined.\")"]},{"cell_type":"markdown","metadata":{"id":"mV6w3ggpfxHd"},"source":["---\n","\n","## 4. 실험 실행"]},{"cell_type":"markdown","metadata":{"id":"ZlmiKbGHfxHd"},"source":["### 실험 E1-v2: Linear + LoRA (Linear Projector)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"id":"hM_71lsBfxHe"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","############################################################\n","#  EXPERIMENT: E1_v2_linear_optimized\n","#  Projector: linear\n","#  LR: stage1=0.001, stage2=5e-05\n","#  Time: 2026-01-21 04:35:36\n","############################################################\n","CheckpointManager: /content/drive/MyDrive/mutsa-02/korean_video_captioning/siglip_study/results3/E1_v2_linear_optimized\n","\n","[START] Starting from scratch\n","\n","[1/5] Building model...\n","\n","Building linear model...\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1c199d28f6d9472591b8e78b3863a723","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/5 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["  PAD: \u003c|endoftext|\u003e (ID: 151643)\n","  EOS: \u003c|im_end|\u003e (ID: 151645)\n","trainable params: 43,646,976 || all params: 8,234,382,336 || trainable%: 0.5301\n","Model built!\n","\n","[2/5] Loading datasets with Vision Caching...\n","\n","[CACHE] Pre-computing vision features for 865 train samples...\n"]},{"name":"stderr","output_type":"stream","text":["Caching train: 100%|██████████| 865/865 [1:11:25\u003c00:00,  4.95s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CACHE] Done! Cached 865 samples.\n","[CACHE] Feature shape: torch.Size([4608, 1024])\n","\n","[CACHE] Pre-computing vision features for 97 val samples...\n"]},{"name":"stderr","output_type":"stream","text":["Caching val: 100%|██████████| 97/97 [07:59\u003c00:00,  4.94s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CACHE] Done! Cached 97 samples.\n","[CACHE] Feature shape: torch.Size([4608, 1024])\n","\n","------------------------------------------------------------\n","CONFIG: linear | Train: 865 | Val: 97\n","Batch: 4x2 | Frames: 8 | MaxLen: 768\n","Stage1: 2ep (LR:0.001) | Stage2: 3ep (LR:5e-05)\n","Warmup: 10% | Grad Clip: 1.0\n","GPU: 10.0GB / 85.2GB\n","------------------------------------------------------------\n","[3/5] Loading SigLIP2 evaluator...\n","Loading SigLIP2: google/siglip2-so400m-patch14-384\n"]},{"name":"stderr","output_type":"stream","text":["Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"]},{"name":"stdout","output_type":"stream","text":["[4/5] Loading Text Metrics evaluator...\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"name":"stdout","output_type":"stream","text":["TextMetricsEvaluator ready\n","\n","[5/5] Starting training...\n","\n","============================================================\n","  STAGE 1: Linear Warm-up (with LR Scheduler)\n","============================================================\n","  Trainable: 4,198,400 params\n","  Total Steps: 434 | Warmup: 43 (10%)\n","  LR: 0.001 | Grad Clip: 1.0\n"]},{"name":"stderr","output_type":"stream","text":["S1 E1/2: 100%|██████████| 217/217 [46:59\u003c00:00, 12.99s/it, loss=1.5438, lr=9.33e-04, mem=14.6G]\n"]},{"name":"stdout","output_type":"stream","text":["\n","[S1 E1] Loss: 1.8293 | LR: 9.33e-04 | Time: 46m59s\n"]},{"name":"stderr","output_type":"stream","text":["S1 E2/2: 100%|██████████| 217/217 [46:53\u003c00:00, 12.97s/it, loss=1.9720, lr=5.90e-04, mem=14.6G]\n"]},{"name":"stdout","output_type":"stream","text":["\n","[S1 E2] Loss: 1.6065 | LR: 5.90e-04 | Time: 46m53s\n","\n","============================================================\n","  Stage 1 Complete! Time: 1h33m53s | Loss: 1.6065\n","============================================================\n","Saved: /content/drive/MyDrive/mutsa-02/korean_video_captioning/siglip_study/results3/E1_v2_linear_optimized/checkpoints/stage1_checkpoint.pt\n","\n","============================================================\n","  STAGE 2: Linear + LoRA (E1-3)\n","============================================================\n","  Trainable: Proj 4,198,400 + LoRA 43,646,976 = 47,845,376\n","  Total Steps: 651 | Warmup: 65\n","  LR: 5e-05 | Grad Clip: 1.0\n"]},{"name":"stderr","output_type":"stream","text":["S2 E1/3: 100%|██████████| 217/217 [47:21\u003c00:00, 13.09s/it, loss=1.6685, lr=4.93e-05, mem=15.1G]\n"]},{"name":"stdout","output_type":"stream","text":["\n","[S2 E1] Train Loss: 1.5239 | LR: 4.93e-05 | Time: 47m21s\n","\n","[Eval] Generating 97 captions...\n"]},{"name":"stderr","output_type":"stream","text":["\rEval:   0%|          | 0/25 [00:00\u003c?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"name":"stdout","output_type":"stream","text":["\n","  [Sample 1]\n","    Gen:  영상은 맑고 화창한 날씨 속에서 한 건물의 외관과 주변 풍경을 담고 있습니다. 카메라는 고정된 상태로, 중앙에 위치한 회사 건물을 중심으로 촬영하고 있으며, 이는 전체적인 구조를...\n","    GT:  영상은 한 도시에 있는 건축물을 담고 있습니다. 중앙에 보이는 건물의 색은 회색이며 직사각형 형태를 가지고 있습니다. 건물의 전면 부분에는 영어로 \"the #\"이 적혀 있고 그 밑...\n","\n","  [Sample 2]\n","    Gen:  영상은 맑고 화창한 날씨 속에서 도심의 한 건물을 중심으로 촬영되었습니다. 카메라는 고정된 상태로, 이 건물과 그 주변을 보여줍니다. 건물 앞에는 넓은 잎사귀가 있는 나무들이 식...\n","    GT:  이 영상은 맑은 날씨에 촬영되었습니다. 화면 중앙에는 \"한국학술진흥재단\" 건물이 자리 잡고 있습니다. 이 건물은 베이지색 외벽과 수직으로 배열된 창문들이 특징이며, 5층 높이로 보...\n","\n","  [Sample 3]\n","    Gen:  영상은 한 사람이 흰색 옷을 입고 길가에 서 있는 장면입니다. 카메라는 고정된 상태로 촬영하고 있습니다. 화면 중앙에는 흰색 상의를 착용한 남자가 보이며, 그 앞쪽으로는 검은색 ...\n","    GT:  이 영상은 2000년대에 촬영된 것으로, 맑은 날 낮의 한옥을 주요 배경으로 촬영된 영상입니다. 영상 전면에 보이는 건물은 한쪽이 열려 있는 밝은 갈색의 대문이 보이며, 검은색 문...\n"]},{"name":"stderr","output_type":"stream","text":["Eval: 100%|██████████| 25/25 [39:39\u003c00:00, 95.19s/it]\n"]},{"name":"stdout","output_type":"stream","text":["\n","[Eval] Done in 39m39s\n","[DEBUG] Empty:0, Diversity:1.00 (97/97 unique)\n","[Eval] Computing SigLIP2...\n"]},{"name":"stderr","output_type":"stream","text":["SigLIP: 100%|██████████| 97/97 [00:49\u003c00:00,  1.96it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[S2 E1] Val SigLIP: 0.1440 | Diversity: 1.00 (BEST!) | Best: 0.1440\n","Saved: /content/drive/MyDrive/mutsa-02/korean_video_captioning/siglip_study/results3/E1_v2_linear_optimized/checkpoints/stage2_epoch1_checkpoint.pt\n","New best! Score: 0.1440\n"]},{"name":"stderr","output_type":"stream","text":["S2 E2/3: 100%|██████████| 217/217 [47:19\u003c00:00, 13.09s/it, loss=1.3886, lr=4.22e-05, mem=15.1G]\n"]},{"name":"stdout","output_type":"stream","text":["\n","[S2 E2] Train Loss: 1.3804 | LR: 4.22e-05 | Time: 47m19s\n","\n","[Eval] Generating 97 captions...\n"]},{"name":"stderr","output_type":"stream","text":["\rEval:   0%|          | 0/25 [00:00\u003c?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","  [Sample 1]\n","    Gen:  이 영상은 맑고 화창한 낮의 광경을 담고 있습니다. 카메라는 고정된 상태로 촬영하고 있으며, 도심 속 건물과 그 주변 풍경이 선명하게 보입니다. 화면 중앙에는 대형 상업 건물이 ...\n","    GT:  영상은 한 도시에 있는 건축물을 담고 있습니다. 중앙에 보이는 건물의 색은 회색이며 직사각형 형태를 가지고 있습니다. 건물의 전면 부분에는 영어로 \"the #\"이 적혀 있고 그 밑...\n","\n","  [Sample 2]\n","    Gen:  영상은 맑고 화창한 낮의 광경을 담고 있습니다. 카메라는 고정된 상태로 촬영하고 있으며, 도심 속 건물과 그 주변 풍경이 중심입니다. 화면 중앙에는 대형 상업 건물이 자리 잡고 ...\n","    GT:  이 영상은 맑은 날씨에 촬영되었습니다. 화면 중앙에는 \"한국학술진흥재단\" 건물이 자리 잡고 있습니다. 이 건물은 베이지색 외벽과 수직으로 배열된 창문들이 특징이며, 5층 높이로 보...\n","\n","  [Sample 3]\n","    Gen:  영상은 맑고 화창한 날씨 속에서 한 사람이 걸어가는 모습을 담고 있습니다. 카메라는 고정된 상태로 촬영되며, 사람의 움직임과 주변 풍경이 선명하게 보입니다. 화면 중앙에는 검은색...\n","    GT:  이 영상은 2000년대에 촬영된 것으로, 맑은 날 낮의 한옥을 주요 배경으로 촬영된 영상입니다. 영상 전면에 보이는 건물은 한쪽이 열려 있는 밝은 갈색의 대문이 보이며, 검은색 문...\n"]},{"name":"stderr","output_type":"stream","text":["Eval:  48%|████▊     | 12/25 [19:34\u003c21:12, 97.85s/it]"]}],"source":["result_e1_v2 = run_experiment(\"E1_v2_linear_optimized\", \"linear\", CONFIG)\n","print(f\"\\nE1-v2 Result: {result_e1_v2}\")"]},{"cell_type":"markdown","metadata":{"id":"ebB_ozwLfxHe"},"source":["---\n","\n","## 5. 결과 요약"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3W6uApqafxHe"},"outputs":[],"source":["print(\"\\n\" + \"=\"*60 + \"\\nE1-v2 (LINEAR PROJECTOR) SUMMARY\\n\" + \"=\"*60)\n","metrics_file = Path(RESULTS_DIR) / \"E1_v2_linear_optimized\" / \"final_metrics.json\"\n","if metrics_file.exists():\n","    with open(metrics_file) as f: result = json.load(f)\n","    print(pd.DataFrame([result]).to_string(index=False))\n","    pd.DataFrame([result]).to_csv(f\"{RESULTS_DIR}/e1_v2_summary.csv\", index=False)\n","else:\n","    print(\"E1-v2: Not completed\")"]},{"cell_type":"markdown","metadata":{"id":"Pvy92jrdfxHe"},"source":["## 6. 모델 로드 (추론용)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qaamWMhBfxHe"},"outputs":[],"source":["def load_trained_model(exp_name, projector_type, config):\n","    ckpt_path = Path(config[\"results_dir\"]) / exp_name / \"checkpoints\" / \"best_model.pt\"\n","    if not ckpt_path.exists():\n","        print(f\"Not found: {ckpt_path}\")\n","        return None, None, None, None\n","    model, vision_encoder, img_proc, tok = build_model(config, projector_type, \"cuda\", ckpt_path)\n","    model.eval()\n","    return model, vision_encoder, img_proc, tok\n","\n","# 사용: model, ve, img_proc, tok = load_trained_model(\"E1_v2_linear_optimized\", \"linear\", CONFIG)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1c199d28f6d9472591b8e78b3863a723":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fc91d1b9fde5472e9891c1f885d8cd8d","IPY_MODEL_8808049995a0440b834b8c6a9a858506","IPY_MODEL_8e1cb6c3e78f4b50a5edb5297ca9a921"],"layout":"IPY_MODEL_dc06311726fc46fe822d8ca556c080de"}},"1f4a6a60f493400e98488e13a90cc229":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8808049995a0440b834b8c6a9a858506":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8dbf600088af4d46b4a03808d9d8ac24","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f1de81531c594045a9ecdcc8e76d3177","value":5}},"8dbf600088af4d46b4a03808d9d8ac24":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e1cb6c3e78f4b50a5edb5297ca9a921":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4c5e6b6bd5b4ab18cd4344b851fdf26","placeholder":"​","style":"IPY_MODEL_1f4a6a60f493400e98488e13a90cc229","value":" 5/5 [01:20\u0026lt;00:00, 13.61s/it]"}},"dc06311726fc46fe822d8ca556c080de":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de705f93bd6c480bbb77507880458664":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfccdef8dc6844b49932c15b5c5ab912":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f1de81531c594045a9ecdcc8e76d3177":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f4c5e6b6bd5b4ab18cd4344b851fdf26":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc91d1b9fde5472e9891c1f885d8cd8d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_de705f93bd6c480bbb77507880458664","placeholder":"​","style":"IPY_MODEL_dfccdef8dc6844b49932c15b5c5ab912","value":"Loading checkpoint shards: 100%"}}}}},"nbformat":4,"nbformat_minor":0}