# 토큰화 분석 및 max_length 결정

> **학습 목표**: 한국어 토큰화 특성 이해, max_length 선택 근거

---

## 1. 토큰화란?

텍스트를 모델이 처리할 수 있는 숫자(토큰 ID)로 변환하는 과정입니다.

```
"귀여운 강아지가 공원에서 뛰어놀고 있습니다."
    │
    ▼ (토큰화)
["귀여운", " 강아지", "가", " 공원", "에서", " 뛰어", "놀고", " 있습니다", "."]
    │
    ▼ (토큰 ID 변환)
[15234, 8923, 102, 7891, 234, 5678, 9012, 3456, 29]
```

---

## 2. 토크나이저 비교 (EDA 결과)

### 2.1 분석 데이터

- **샘플 수**: 2,000개 (전체 데이터셋에서 샘플링)
- **분석 대상**: LLaMA, Qwen3-VL-8B, tiktoken (GPT-4)

### 2.2 토큰 수 통계

| 토크나이저 | 어휘 크기 | 평균 | 중앙값 | 최소 | **최대** | 표준편차 |
|------------|----------|------|--------|------|------|----------|
| LLaMA (Vicuna) | 32,000 | 931.9 | 899.0 | 761 | 1572 | 111.7 |
| **Qwen3-VL-8B** | 151,936 | **401.9** | 388.0 | 311 | **708** | 50.7 |
| tiktoken (GPT-4) | 100,000 | 574.3 | 553.0 | 462 | 1009 | 72.0 |

### 2.3 핵심 발견

**Qwen3가 LLaMA 대비 2.3배 효율적!**

```
동일한 한국어 캡션:
  LLaMA:  931.9 토큰 (평균)
  Qwen3:  401.9 토큰 (평균)
  
효율성 = 931.9 / 401.9 = 2.32배
```

---

## 3. 왜 Qwen3가 한국어에 효율적인가?

### 3.1 어휘 크기 차이

| 토크나이저 | 어휘 크기 | 한국어 토큰 |
|------------|----------|------------|
| LLaMA | 32K | ~3,000 |
| Qwen3 | 152K | ~30,000+ |

Qwen3는 한국어 전용 토큰이 훨씬 많습니다.

### 3.2 토큰화 예시

```
입력: "대한민국"

LLaMA 토큰화:
  ["대", "한", "민", "국"] → 4 토큰

Qwen3 토큰화:
  ["대한민국"] → 1 토큰
```

### 3.3 시각화

```
토큰 수 분포:

LLaMA:
  600  800  1000  1200  1400  1600
   │────────────────────────────│
          ████████████
        ████████████████
      ██████████████████████
        ████████████████
          ████████████
   
Qwen3:
  300   400   500   600   700   800
   │────────────────────────────│
              ████
            ████████
          ████████████████
            ████████
              ████
```

---

## 4. max_length 초과 분석

### 4.1 임계값별 초과 비율

| 토크나이저 | >512 | >768 | >1024 | >2048 |
|------------|------|------|-------|-------|
| LLaMA | 100% | 93.2% | 15.4% | 0% |
| **Qwen3** | **4.9%** | **0%** | 0% | 0% |
| tiktoken | 88.5% | 12.3% | 0% | 0% |

### 4.2 Qwen3 상세 분포

```
토큰 수 범위별 샘플 수:

~400:   ████████████████████████████ 48%
400~500: ██████████████████████ 38%
500~600: ████████ 10%
600~708: ████ 4%
>708:    없음
```

---

## 5. max_length 결정 근거

### 5.1 요구사항

1. **데이터 손실 최소화**: truncation으로 인한 정보 손실 방지
2. **메모리 효율**: 불필요하게 긴 시퀀스 방지
3. **안전 마진**: 약간의 여유

### 5.2 분석

| max_length | Qwen3 커버리지 | 메모리 | 권장 |
|------------|---------------|--------|------|
| 512 | 95.1% | 낮음 | △ |
| 640 | ~98% | 중간 | ○ |
| **768** | **~100%** | 중간 | **◎** |
| 1024 | 100% | 높음 | △ (과잉) |

### 5.3 결정

**max_length = 768 권장**

- Qwen3 최대 토큰 수: 708
- 여유 마진: 60 토큰 (~8%)
- 모든 샘플 truncation 없이 처리 가능

---

## 6. 코드에서의 적용

### 6.1 데이터셋 설정

```python
CONFIG = {
    # 변경 전
    # "max_length": 512,  # 4.9% 샘플 truncation
    
    # 변경 후
    "max_length": 768,  # 모든 샘플 커버
}
```

### 6.2 토크나이저 사용

```python
text_inputs = tokenizer(
    caption,
    max_length=config["max_length"],  # 768
    padding="max_length",
    truncation=True,  # 혹시 모를 예외 처리
    return_tensors="pt"
)
```

---

## 7. VLM에서의 시퀀스 길이

### 7.1 전체 시퀀스 구성

```
LLM 입력 시퀀스:

┌─────────────────────────────────────────────────────────┐
│  Vision Tokens  │  Text Prompt  │  Generated Caption    │
│   (Projector)   │               │                       │
├─────────────────┼───────────────┼───────────────────────┤
│  64~4608 tokens │  ~20 tokens   │  ~768 tokens (max)    │
└─────────────────┴───────────────┴───────────────────────┘
```

### 7.2 Projector별 총 시퀀스 길이

| Projector | Vision | Prompt | Caption | Total |
|-----------|--------|--------|---------|-------|
| Linear/MLP | 4608 | 20 | 768 | **5396** |
| C-Abstractor | 64 | 20 | 768 | **852** |
| Perceiver | 64 | 20 | 768 | **852** |

**토큰 압축 Projector의 장점**:
- 시퀀스 길이 6배 감소
- LLM 추론 속도 향상
- Attention 메모리 절약

---

## 8. 메모리 영향

### 8.1 Attention 메모리 계산

Self-Attention의 메모리 복잡도: $O(n^2)$

```
시퀀스 길이 n에 대한 Attention 메모리:

n = 5396 (Linear):     5396² = 29,118,416 요소
n = 852 (Perceiver):   852² = 725,904 요소

비율: 29,118,416 / 725,904 = 40배 차이!
```

### 8.2 실제 GPU 메모리 (A100 80GB)

| 설정 | Vision Tokens | max_length | 예상 메모리 |
|------|---------------|------------|------------|
| Linear + 512 | 4608 | 512 | ~35GB |
| Linear + 768 | 4608 | 768 | ~40GB |
| Perceiver + 512 | 64 | 512 | ~20GB |
| Perceiver + 768 | 64 | 768 | ~22GB |

---

## 9. 요약

### 9.1 핵심 수치

| 항목 | 값 | 비고 |
|------|-----|------|
| Qwen3 평균 토큰 | 401.9 | LLaMA 대비 2.3배 효율 |
| Qwen3 최대 토큰 | 708 | 전체 데이터셋 기준 |
| 권장 max_length | **768** | 708 + 여유 마진 |
| 512 초과 비율 | 4.9% | → 768로 0% |

### 9.2 노트북 수정 사항

```python
# configs in 01_person_a.ipynb, 02_person_b.ipynb, 03_person_c.ipynb
CONFIG = {
    ...
    "max_length": 768,  # 기존 512 → 768
    ...
}
```

---

## 참고 자료

- [EDA 분석 결과](../../../local_train/eda/reports/04_tokenizer_comparison.md)
- [Qwen Tokenizer](https://huggingface.co/Qwen/Qwen3-8B)
- [BPE 토큰화](https://arxiv.org/abs/1508.07909) - Byte Pair Encoding
